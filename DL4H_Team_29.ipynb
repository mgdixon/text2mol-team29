{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "gpuType": "A100",
      "toc_visible": true,
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mgdixon/text2mol-team29/blob/main/DL4H_Team_29.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# CS598: DLH - Project Draft\n",
        "\n",
        "### Team 29\n",
        "* Antony Vo (avvo3@illinois.edu),\n",
        "* Anirudh Prasad (ap73@illinois.edu),\n",
        "* Martin Dixon (mgdixon2@illinois.edu)\n",
        "\n",
        "### Date: April 14th, 2024"
      ],
      "metadata": {
        "id": "mVK08Ae8j06z"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Rubric for Draft (20 Points)\n",
        "**TODO:** *Delete this section before turning in*\n",
        "\n",
        "\n",
        "Jupyter Notebook (both .PDF and .ipynb files)\n",
        "\n",
        "You need to use the report template and fill out the following sections, each of which we will score based on the clarity and appropriateness of your writing (percentage of total grade for each component shown). All the information must be in the Jupyter notebook.\n",
        "\n",
        "* Introduction (2)\n",
        "  * A clear, high-level description of what the original paper is about and what is the contribution of it\n",
        "* Scope of reproducibility (2)\n",
        "* Methodology (8)\n",
        "  * Data\n",
        "    * Data descriptions\n",
        "    * Implementation code\n",
        "  * Model\n",
        "    * Model descriptions\n",
        "    * Implementation code\n",
        "  * Training\n",
        "    * Computational requirements\n",
        "    * Implementation code\n",
        "  * Evaluation\n",
        "    * Metrics descriptions\n",
        "    * Implementation code\n",
        "* Results (8)\n",
        "  * Results\n",
        "  * Analyses\n",
        "  * Plans"
      ],
      "metadata": {
        "id": "V-CWVXD_asek"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Before you use this template\n",
        "**TODO:** *Delete this section before turning in*\n",
        "\n",
        "[Link to Original Template](https://colab.research.google.com/drive/1MGxB_J2TvhAANcQG8VNMvQp1QdQrcxWb?usp=sharing)\n",
        "\n",
        "\n",
        "This template is just a recommended template for project Report. It only considers the general type of research in our paper pool. Feel free to edit it to better fit your project. You will iteratively update the same notebook submission for your draft and the final submission. Please check the project rubriks to get a sense of what is expected in the template.\n",
        "\n",
        "---\n",
        "\n",
        "# FAQ and Attentions\n",
        "* Copy and move this template to your Google Drive. Name your notebook by your team ID (upper-left corner). Don't eidt this original file.\n",
        "* This template covers most questions we want to ask about your reproduction experiment. You don't need to exactly follow the template, however, you should address the questions. Please feel free to customize your report accordingly.\n",
        "* any report must have run-able codes and necessary annotations (in text and code comments).\n",
        "* The notebook is like a demo and only uses small-size data (a subset of original data or processed data), the entire runtime of the notebook including data reading, data process, model training, printing, figure plotting, etc,\n",
        "must be within 8 min, otherwise, you may get penalty on the grade.\n",
        "  * If the raw dataset is too large to be loaded  you can select a subset of data and pre-process the data, then, upload the subset or processed data to Google Drive and load them in this notebook.\n",
        "  * If the whole training is too long to run, you can only set the number of training epoch to a small number, e.g., 3, just show that the training is runable.\n",
        "  * For results model validation, you can train the model outside this notebook in advance, then, load pretrained model and use it for validation (display the figures, print the metrics).\n",
        "* The post-process is important! For post-process of the results,please use plots/figures. The code to summarize results and plot figures may be tedious, however, it won't be waste of time since these figures can be used for presentation. While plotting in code, the figures should have titles or captions if necessary (e.g., title your figure with \"Figure 1. xxxx\")\n",
        "* There is not page limit to your notebook report, you can also use separate notebooks for the report, just make sure your grader can access and run/test them.\n",
        "* If you use outside resources, please refer them (in any formats). Include the links to the resources if necessary."
      ],
      "metadata": {
        "id": "j01aH0PR4Sg-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Introduction - Original Instructions\n",
        "\n",
        "**TODO:** *Delete this section before turning in*\n",
        "\n",
        "This is an introduction to your report, you should edit this text/mardown section to compose. In this text/markdown, you should introduce:\n",
        "\n",
        "*   Background of the problem\n",
        "  * what type of problem: disease/readmission/mortality prediction,  feature engineeing, data processing, etc\n",
        "  * what is the importance/meaning of solving the problem\n",
        "  * what is the difficulty of the problem\n",
        "  * the state of the art methods and effectiveness.\n",
        "*   Paper explanation\n",
        "  * what did the paper propose\n",
        "  * what is the innovations of the method\n",
        "  * how well the proposed method work (in its own metrics)\n",
        "  * what is the contribution to the reasearch regime (referring the Background above, how important the paper is to the problem)."
      ],
      "metadata": {
        "id": "vFgJxJCySBci"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Introduction\n",
        "\n",
        "This work presented below is in cross-modal molecule retrieval. That is, the mapping of chemical descriptions to molecular graphs. Solving this problem has a lot of benefits namely:\n",
        "* Improving molecule discovery and design\n",
        "* Facilitating semantic-level search between natural language and molecules\n",
        "* Enabling natural language query expansion in chemistry information retrieval systems\n",
        "\n",
        "Text2Mol [1] develops a new cross-modal information retrieval task, where molecules are retrieved directly from natural language descriptions.\n",
        "\n",
        "E.g. A description of water (chemical formula being H20) as 2x Hydrogen Atoms bonded to an Oxygen should return a molecular graph. It develops a cross-modal embedding approach using a text encoder (e.g., SciBERT) and a molecule encoder (e.g., GCN and MLP) to create aligned semantic representations. The system uses cosine similarity for ranking molecules and leverages cross-modal attention to extract interpretable associations between text and molecule substructures. Overall the paper demonstrates the effectiveness of an ensemble approach that combines different molecule encoder architectures.\n",
        "\n",
        "The majority of the code below was taken and modified from the [GitHub](https://github.com/cnedwards/text2mol/tree/master/code) repository under the [MIT License](https://opensource.org/license/mit).\n",
        "\n"
      ],
      "metadata": {
        "id": "MQ0sNuMePBXx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Scope of Reproducibility:\n",
        "\n",
        "**TODO:** *Complete this section from our initial doc*\n",
        "\n",
        "List hypotheses from the paper you will test and the corresponding experiments you will run.\n",
        "\n",
        "\n",
        "1.   Hypothesis 1: xxxxxxx\n",
        "2.   Hypothesis 2: xxxxxxx\n",
        "\n",
        "You can insert images in this notebook text, [see this link](https://stackoverflow.com/questions/50670920/how-to-insert-an-inline-image-in-google-colaboratory-from-google-drive) and example below:\n",
        "\n",
        "![sample_image.png](https://drive.google.com/uc?export=view&id=1g2efvsRJDxTxKz-OY3loMhihrEUdBxbc)\n"
      ],
      "metadata": {
        "id": "uygL9tTPSVHB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "You can also use code to display images, see the code below.\n",
        "\n",
        "The images must be saved in Google Drive first.\n"
      ],
      "metadata": {
        "id": "LM4WUjz64C3B"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Methodology\n",
        "\n",
        "Below is our methodology for reproducing Text2Mol. We start with prerequisites. That is, libraries, imports, constants, etc. We then download our pretrained models and our data from Google Cloud Storage."
      ],
      "metadata": {
        "id": "xWAHJ_1CdtaA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Libraries, Imports and Setup"
      ],
      "metadata": {
        "id": "QcUIq2RSS730"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q torch-geometric"
      ],
      "metadata": {
        "id": "o4TY4nZHceiQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# import necessary packages\n",
        "import numpy as np\n",
        "from google.colab import drive\n",
        "\n",
        "# Text2Mol Imports\n",
        "import os\n",
        "import shutil\n",
        "import time\n",
        "import random\n",
        "import csv\n",
        "import math\n",
        "import os.path as osp\n",
        "import zipfile\n",
        "\n",
        "from functools import partial\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "# Torch Libraries\n",
        "import torch\n",
        "from torch import nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torch.nn import Transformer\n",
        "from torch.nn import TransformerEncoder, TransformerEncoderLayer, TransformerDecoder, TransformerDecoderLayer\n",
        "\n",
        "# Tokenizer\n",
        "import tokenizers\n",
        "from tokenizers import Tokenizer\n",
        "\n",
        "# Bert Model\n",
        "from transformers import BertTokenizerFast, BertModel\n"
      ],
      "metadata": {
        "id": "yu61Jp1xrnKk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Torch Geometric - used by GCN\n",
        "from torch_geometric.data import download_url, Data\n",
        "from torch_geometric.data import Dataset as GeoDataset\n",
        "from torch_geometric.data import DataLoader as GeoDataLoader\n",
        "from torch_geometric.data import Data, Batch\n",
        "from torch_geometric.nn import GCNConv\n",
        "from torch_geometric.nn import global_mean_pool"
      ],
      "metadata": {
        "id": "NcJ8CHuIcFCE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Constants\n",
        "\n",
        "The constants below are used by Text2Mol. For clarity, pulling them upfront in the notebook for visibility."
      ],
      "metadata": {
        "id": "AJEqSngPHIZ3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# dir and function to load raw data\n",
        "RAW_DATA_DIR = 'text2mol-master/data/'\n",
        "\n",
        "# model filenames\n",
        "GCN_MODEL_PATH = 'GCN_final_weights.40.pt'\n",
        "MLP_MODEL_PATH = 'MLP_final_weights.40.pt'\n",
        "TRANSFORMER_MODEL_PATH = 'transformer_final_weights.40.pt'\n",
        "\n",
        "# Constants for training\n",
        "DEVICE = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "BATCH_SIZE = 32\n",
        "NUM_WORKERS = 1\n",
        "TEXT_TRUNC_LENGTH = 256\n",
        "\n",
        "# For the purpose of this file, we only run 2 Epochs\n",
        "NUM_EPOCHS = 2\n",
        "\n",
        "# Train the model flag\n",
        "TRAIN_MLP = True\n",
        "TRAIN_GCN = True\n",
        "TRAIN_TRANSFORMER = False\n"
      ],
      "metadata": {
        "id": "Gq4jCR18HHVI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Fetch Data from GCS\n",
        "\n",
        "Our group decided to store the files on GCS and utilize the local colab filesystem for everything else. This seemed faster than Google Drive, as there is one file `mol_graph.zip` that consists of thousands of tiny text files. These perform very badly with remote filesystem overhead. Additionally, we wished to avoid having to authenticate to access files. Instead we download everything to the local disk for speed."
      ],
      "metadata": {
        "id": "kwovrElfSZid"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# The original github repo\n",
        "!wget --no-verbose https://storage.googleapis.com/team29-text2mol/text2mol-master.zip\n",
        "!unzip -q text2mol-master.zip\n",
        "\n",
        "# The trained MLP, GCN and transformer models from 40 epochs each\n",
        "!wget --no-verbose https://storage.googleapis.com/team29-text2mol/models/MLP_final_weights.40.pt\n",
        "!wget --no-verbose https://storage.googleapis.com/team29-text2mol/models/GCN_final_weights.40.pt\n",
        "!wget --no-verbose https://storage.googleapis.com/team29-text2mol/models/transformer_final_weights.40.pt\n",
        "\n",
        "# The multi-head attention weights from the transformer model\n",
        "!wget --no-verbose https://storage.googleapis.com/team29-text2mol/models/mha_weights.pkl\n",
        "\n",
        "# Fetch the embeddings & untar\n",
        "!wget --no-verbose https://storage.googleapis.com/team29-text2mol/models/GCN_embeddings.tar.gz\n",
        "!mkdir GCN_embeddings\n",
        "!cd GCN_embeddings && tar xfz ../GCN_embeddings.tar.gz\n",
        "\n",
        "!wget --no-verbose https://storage.googleapis.com/team29-text2mol/models/MLP_embeddings.tar.gz\n",
        "!mkdir MLP_embeddings\n",
        "!cd MLP_embeddings && tar xfz ../MLP_embeddings.tar.gz\n"
      ],
      "metadata": {
        "id": "XWcyPrm6Suo_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##  Data - Instructions\n",
        "\n",
        "**TODO:** *Delete this section prior to uploading*\n",
        "\n",
        "Data includes raw data (MIMIC III tables), descriptive statistics (our homework questions), and data processing (feature engineering).\n",
        "  * Source of the data: where the data is collected from; if data is synthetic or self-generated, explain how. If possible, please provide a link to the raw datasets.\n",
        "  * Statistics: include basic descriptive statistics of the dataset like size, cross validation split, label distribution, etc.\n",
        "  * Data process: how do you munipulate the data, e.g., change the class labels, split the dataset to train/valid/test, refining the dataset.\n",
        "  * Illustration: printing results, plotting figures for illustration.\n",
        "  * You can upload your raw dataset to Google Drive and mount this Colab to the same directory. If your raw dataset is too large, you can upload the processed dataset and have a code to load the processed dataset."
      ],
      "metadata": {
        "id": "2NbPHUTMbkD3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data\n",
        "\n",
        "The data used in Text2Mol was created by the authors by utilizing ChEBI (Chemical Entities of Biological Interest) **TODO: INSERT REFERENCE** and PubChem. This was done by initially gathering 102,980 compound-description pairs from chemical annotations. The \"ChEBI-20\" dataset, which contains more than 20 words in its descriptions, was then produced from this data with 33,010 pairs. Text2Mol uses this dataset for training, validation, and testing in an 80%/10%/10% split respectively.\n",
        "\n",
        "The data is available of [GitHub](https://github.com/cnedwards/text2mol/tree/master/data).\n"
      ],
      "metadata": {
        "id": "o4-ytho7Hk4S"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "**Data generation**\n",
        "\n",
        "There are twe data generators used in the project. The first one is used for the MLP and GCN models. The second is used for the Attention model. The Attention flavor differs by the use of a coin toss to set whether the actual description is used or a random one. The authors duplicated code heavily in their notebooks which can make it hard to read."
      ],
      "metadata": {
        "id": "XzVUQS0CHry0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class GenerateData():\n",
        "  def __init__(self, path_train, path_val, path_test, path_molecules, path_token_embs):\n",
        "    self.path_train = path_train\n",
        "    self.path_val = path_val\n",
        "    self.path_test = path_test\n",
        "    self.path_molecules = path_molecules\n",
        "    self.path_token_embs = path_token_embs\n",
        "\n",
        "    self.text_trunc_length = TEXT_TRUNC_LENGTH\n",
        "\n",
        "    self.prep_text_tokenizer()\n",
        "\n",
        "    self.load_substructures()\n",
        "\n",
        "    self.batch_size = BATCH_SIZE\n",
        "\n",
        "    self.store_descriptions()\n",
        "\n",
        "  def load_substructures(self):\n",
        "    self.molecule_sentences = {}\n",
        "    self.molecule_tokens = {}\n",
        "\n",
        "    total_tokens = set()\n",
        "    self.max_mol_length = 0\n",
        "    with open(self.path_molecules) as f:\n",
        "      for line in f:\n",
        "        spl = line.split(\":\")\n",
        "        cid = spl[0]\n",
        "        tokens = spl[1].strip()\n",
        "        self.molecule_sentences[cid] = tokens\n",
        "        t = tokens.split()\n",
        "        total_tokens.update(t)\n",
        "        size = len(t)\n",
        "        if size > self.max_mol_length: self.max_mol_length = size\n",
        "\n",
        "    self.token_embs = np.load(self.path_token_embs, allow_pickle = True)[()]\n",
        "\n",
        "\n",
        "  def prep_text_tokenizer(self):\n",
        "    self.text_tokenizer = BertTokenizerFast.from_pretrained(\"allenai/scibert_scivocab_uncased\")\n",
        "\n",
        "  # Refactored version that's much shorter\n",
        "  def read_file(self, file_path):\n",
        "    cids = []\n",
        "    with open(file_path) as f:\n",
        "        reader = csv.DictReader(f, delimiter=\"\\t\", quoting=csv.QUOTE_NONE, fieldnames=['cid', 'mol2vec', 'desc'])\n",
        "        for line in reader:\n",
        "            self.descriptions[line['cid']] = line['desc']\n",
        "            self.mols[line['cid']] = line['mol2vec']\n",
        "            cids.append(line['cid'])\n",
        "    return cids\n",
        "\n",
        "  def store_descriptions(self):\n",
        "    self.descriptions = {}\n",
        "    self.mols = {}\n",
        "    self.training_cids = self.read_file(self.path_train)\n",
        "    self.validation_cids = self.read_file(self.path_val)\n",
        "    self.test_cids = self.read_file(self.path_test)\n",
        "\n",
        "  # Refactored version\n",
        "  def generate_examples(self, cids):\n",
        "    \"\"\"Yields examples.\"\"\"\n",
        "\n",
        "    np.random.shuffle(cids)\n",
        "\n",
        "    for cid in cids:\n",
        "      text_input = self.text_tokenizer(self.descriptions[cid], truncation=True, max_length=self.text_trunc_length,\n",
        "                                        padding='max_length', return_tensors = 'np')\n",
        "\n",
        "      yield {\n",
        "          'cid': cid,\n",
        "          'input': {\n",
        "              'text': {\n",
        "                'input_ids': text_input['input_ids'].squeeze(),\n",
        "                'attention_mask': text_input['attention_mask'].squeeze(),\n",
        "                'original_text': self.descriptions[cid]\n",
        "              },\n",
        "              'molecule' : {\n",
        "                    'mol2vec' : np.fromstring(self.mols[cid], sep = \" \"),\n",
        "                    'cid' : cid\n",
        "              },\n",
        "          },\n",
        "      }\n",
        "\n",
        "  def generate_examples_train(self):\n",
        "    yield from self.generate_examples(self.training_cids)\n",
        "\n",
        "  def generate_examples_val(self):\n",
        "    yield from self.generate_examples(self.validation_cids)\n",
        "\n",
        "  def generate_examples_test(self):\n",
        "    yield from self.generate_examples(self.test_cids)\n",
        "\n",
        "  def generate_examples_custom(self, custom_cids):\n",
        "    yield from self.generate_examples(custom_cids)\n",
        "\n"
      ],
      "metadata": {
        "id": "xpJsHDJOlT5N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "\n",
        "**The Attention model Generator**\n",
        "\n",
        "This is a slightly different generator used by the Attention model. As mentioned above, it uses a coin toss to decide which text description is used."
      ],
      "metadata": {
        "id": "uiOerC5o3v0W"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# TODO: Refactor this generator and the one above to have inheritance\n",
        "class GenerateDataAttention():\n",
        "  def __init__(self, path_train, path_val, path_test, path_molecules, path_token_embs):\n",
        "    self.path_train = path_train\n",
        "    self.path_val = path_val\n",
        "    self.path_test = path_test\n",
        "    self.path_molecules = path_molecules\n",
        "    self.path_token_embs = path_token_embs\n",
        "\n",
        "    self.mol_trunc_length = 512\n",
        "    self.text_trunc_length = 256\n",
        "\n",
        "    self.prep_text_tokenizer()\n",
        "\n",
        "    self.load_substructures()\n",
        "\n",
        "    self.batch_size = BATCH_SIZE\n",
        "\n",
        "    self.store_descriptions()\n",
        "\n",
        "  def load_substructures(self):\n",
        "    self.molecule_sentences = {}\n",
        "    self.molecule_tokens = {}\n",
        "\n",
        "    total_tokens = set()\n",
        "    self.max_mol_length = 0\n",
        "    with open(self.path_molecules) as f:\n",
        "      for line in f:\n",
        "        spl = line.split(\":\")\n",
        "        cid = spl[0]\n",
        "        tokens = spl[1].strip()\n",
        "        self.molecule_sentences[cid] = tokens\n",
        "        t = tokens.split()\n",
        "        total_tokens.update(t)\n",
        "        size = len(t)\n",
        "        if size > self.max_mol_length: self.max_mol_length = size\n",
        "\n",
        "\n",
        "    self.token_embs = np.load(self.path_token_embs, allow_pickle = True)[()]\n",
        "\n",
        "\n",
        "\n",
        "  def prep_text_tokenizer(self):\n",
        "    self.text_tokenizer = BertTokenizerFast.from_pretrained(\"allenai/scibert_scivocab_uncased\")\n",
        "\n",
        "\n",
        "  def store_descriptions(self):\n",
        "    self.descriptions = {}\n",
        "    self.mols = {}\n",
        "    self.training_cids = []\n",
        "    self.validation_cids = []\n",
        "    self.test_cids = []\n",
        "\n",
        "    def process_file(filepath, cids_list):\n",
        "        with open(filepath) as f:\n",
        "            reader = csv.DictReader(f, delimiter=\"\\t\", quoting=csv.QUOTE_NONE, fieldnames=['cid', 'mol2vec', 'desc'])\n",
        "            for n, line in enumerate(reader):\n",
        "                self.descriptions[line['cid']] = line['desc']\n",
        "                self.mols[line['cid']] = line['mol2vec']\n",
        "                cids_list.append(line['cid'])\n",
        "\n",
        "    process_file(self.path_train, self.training_cids)\n",
        "    process_file(self.path_val, self.validation_cids)\n",
        "    process_file(self.path_test, self.test_cids)\n",
        "\n",
        "  #transformers can't take array with full attention so have to pad a 0...\n",
        "  def padarray(self, A, size, value=0):\n",
        "      t = size - len(A)\n",
        "      return np.pad(A, pad_width=(0, t), mode='constant', constant_values = value)\n",
        "\n",
        "  def generate_examples(self, cids):\n",
        "    \"\"\"Yields examples.\"\"\"\n",
        "\n",
        "    np.random.shuffle(cids)\n",
        "\n",
        "    for cid in cids:\n",
        "        label = np.random.randint(2)\n",
        "        rand_cid = np.random.choice(cids)\n",
        "\n",
        "        # Choose description based on label\n",
        "        if label:\n",
        "            description = self.descriptions[cid]\n",
        "        else:\n",
        "            description = self.descriptions[rand_cid]\n",
        "\n",
        "        text_input = self.text_tokenizer(description, truncation=True, max_length=self.text_trunc_length - 1,\n",
        "                                         padding='max_length', return_tensors='np')\n",
        "\n",
        "        text_ids = self.padarray(text_input['input_ids'].squeeze(), self.text_trunc_length)\n",
        "        text_mask = self.padarray(text_input['attention_mask'].squeeze(), self.text_trunc_length)\n",
        "\n",
        "        yield {\n",
        "            'cid': cid,\n",
        "            'input': {\n",
        "                'text': {\n",
        "                    'input_ids': text_ids,\n",
        "                    'attention_mask': text_mask,\n",
        "                },\n",
        "                'molecule': {\n",
        "                    'mol2vec': np.fromstring(self.mols[cid], sep=\" \"),\n",
        "                    'cid': cid\n",
        "                },\n",
        "            },\n",
        "            'label': label\n",
        "        }\n",
        "\n",
        "\n",
        "  def generate_examples_train(self):\n",
        "    return self.generate_examples(self.training_cids)\n",
        "\n",
        "  def generate_examples_val(self):\n",
        "    return self.generate_examples(self.validation_cids)\n",
        "\n",
        "  def generate_examples_test(self):\n",
        "    return self.generate_examples(self.test_cids)\n",
        "\n",
        "  def generate_examples_custom(self, custom_cids):\n",
        "    yield from self.generate_examples(custom_cids)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "fPevTKRg3Xul"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "\n",
        "**Graph Dataset**\n",
        "\n",
        "This code creates a custom graph Dataset & Dataloader for the chemical ids extracted from the file `mol_graph.zip`."
      ],
      "metadata": {
        "id": "9iHTyMtRVMtB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "class MoleculeGraphDataset(GeoDataset):\n",
        "    def __init__(self, root, cids, data_path, gt, transform=None, pre_transform=None):\n",
        "        self.cids = cids\n",
        "        self.data_path = data_path\n",
        "        self.gt = gt\n",
        "        super(MoleculeGraphDataset, self).__init__(root, transform, pre_transform)\n",
        "\n",
        "        self.idx_to_cid = {}\n",
        "        i = 0\n",
        "        for raw_path in self.raw_paths:\n",
        "            cid = int(raw_path.split('/')[-1][:-6])\n",
        "            self.idx_to_cid[i] = cid\n",
        "            i += 1\n",
        "\n",
        "    @property\n",
        "    def raw_file_names(self):\n",
        "        return [cid + \".graph\" for cid in self.cids]\n",
        "\n",
        "    @property\n",
        "    def processed_file_names(self):\n",
        "        return ['data_{}.pt'.format(cid) for cid in self.cids]\n",
        "\n",
        "    def download(self):\n",
        "        # Download to `self.raw_dir`.\n",
        "        shutil.copy(self.data_path, os.path.join(self.raw_dir, \"/mol_graphs.zip\"))\n",
        "\n",
        "    def process_graph(self, raw_path):\n",
        "      edge_index  = []\n",
        "      x = []\n",
        "      with open(raw_path, 'r') as f:\n",
        "        next(f)\n",
        "        for line in f: #edges\n",
        "          if line != \"\\n\":\n",
        "            edge = *map(int, line.split()),\n",
        "            edge_index.append(edge)\n",
        "          else:\n",
        "            break\n",
        "        next(f)\n",
        "        for line in f: #get mol2vec features:\n",
        "          substruct_id = line.strip().split()[-1]\n",
        "          if substruct_id in self.gt.token_embs:\n",
        "            x.append(self.gt.token_embs[substruct_id])\n",
        "          else:\n",
        "            x.append(self.gt.token_embs['UNK'])\n",
        "\n",
        "        return torch.LongTensor(edge_index).T, torch.FloatTensor(x)\n",
        "\n",
        "\n",
        "\n",
        "    def process(self):\n",
        "\n",
        "        with zipfile.ZipFile(os.path.join(self.raw_dir, \"/mol_graphs.zip\"), 'r') as zip_ref:\n",
        "            zip_ref.extractall(self.raw_dir)\n",
        "\n",
        "\n",
        "        i = 0\n",
        "        for raw_path in self.raw_paths:\n",
        "            # Read data from `raw_path`.\n",
        "\n",
        "            cid = int(raw_path.split('/')[-1][:-6])\n",
        "\n",
        "            edge_index, x = self.process_graph(raw_path)\n",
        "            data = Data(x=x, edge_index = edge_index)\n",
        "\n",
        "            if self.pre_filter is not None and not self.pre_filter(data):\n",
        "                continue\n",
        "\n",
        "            if self.pre_transform is not None:\n",
        "                data = self.pre_transform(data)\n",
        "\n",
        "            torch.save(data, osp.join(self.processed_dir, 'data_{}.pt'.format(cid)))\n",
        "            i += 1\n",
        "\n",
        "    def len(self):\n",
        "        return len(self.processed_file_names)\n",
        "\n",
        "    def get(self, idx):\n",
        "        data = torch.load(osp.join(self.processed_dir, 'data_{}.pt'.format(self.idx_to_cid[idx])))\n",
        "        return data\n",
        "\n",
        "    def get_cid(self, cid):\n",
        "        data = torch.load(osp.join(self.processed_dir, 'data_{}.pt'.format(cid)))\n",
        "        return data\n",
        "\n",
        "#To get specific lists...\n",
        "\n",
        "class CustomGraphCollater(object):\n",
        "    def __init__(self, dataset, follow_batch = [], exclude_keys = []):\n",
        "        self.follow_batch = follow_batch\n",
        "        self.exclude_keys = exclude_keys\n",
        "        self.dataset = dataset\n",
        "\n",
        "    def collate(self, batch):\n",
        "        elem = batch[0]\n",
        "        if isinstance(elem, Data):\n",
        "            return Batch.from_data_list(batch)\n",
        "\n",
        "        raise TypeError('DataLoader found invalid type: {}'.format(type(elem)))\n",
        "\n",
        "    def __call__(self, cids):\n",
        "\n",
        "        return self.collate([self.dataset.get_cid(int(cid)) for cid in cids])\n",
        "\n",
        "class CustomGraphCollaterTransformer(object):\n",
        "    def __init__(self, dataset, mask_len, follow_batch = [], exclude_keys = []):\n",
        "        self.follow_batch = follow_batch\n",
        "        self.exclude_keys = exclude_keys\n",
        "        self.dataset = dataset\n",
        "        self.mask_len = mask_len\n",
        "        self.mask_indices = np.array(range(mask_len))\n",
        "\n",
        "    def generate_mask(self, sz):\n",
        "        rv = torch.zeros((self.mask_len), dtype = torch.bool)\n",
        "        rv = rv.masked_fill(torch.BoolTensor(self.mask_indices < sz), bool(1)) #pytorch transformer input version\n",
        "        rv[-1] = 0 #set last value to 0 because pytorch can't handle all 1s\n",
        "        return rv\n",
        "\n",
        "    def get_masks(self, batch):\n",
        "      return torch.stack([self.generate_mask(b.x.shape[0]) for b in batch])\n",
        "\n",
        "    def collate(self, batch):\n",
        "        elem = batch[0]\n",
        "        if isinstance(elem, Data):\n",
        "            return Batch.from_data_list(batch)\n",
        "\n",
        "        raise TypeError('DataLoader found invalid type: {}'.format(type(elem)))\n",
        "\n",
        "    def __call__(self, cids):\n",
        "\n",
        "        tmp = [self.dataset.get_cid(int(cid)) for cid in cids]\n",
        "        return self.collate(tmp), self.get_masks(tmp)"
      ],
      "metadata": {
        "id": "GcW8ifnXYkzf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Dataset(Dataset):\n",
        "  'Characterizes a dataset for PyTorch'\n",
        "  def __init__(self, gen, length):\n",
        "      'Initialization'\n",
        "\n",
        "      self.gen = gen\n",
        "      self.it = iter(self.gen())\n",
        "\n",
        "      self.length = length\n",
        "\n",
        "  def __len__(self):\n",
        "      'Denotes the total number of samples'\n",
        "      return self.length\n",
        "\n",
        "\n",
        "  def __getitem__(self, index):\n",
        "      'Generates one sample of data'\n",
        "\n",
        "      try:\n",
        "        ex = next(self.it)\n",
        "      except StopIteration:\n",
        "        self.it = iter(self.gen())\n",
        "        ex = next(self.it)\n",
        "\n",
        "      X = ex['input']\n",
        "      y = 1\n",
        "\n",
        "      return X, y"
      ],
      "metadata": {
        "id": "2f3p9IcYyuYo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "\n",
        "**Loading & Processing Data**\n",
        "\n",
        "Below we load and process the molecules and the graph data. This code takes several minutes to run. Afterwards we show some statistics on the dataset."
      ],
      "metadata": {
        "id": "M1Lwc5QVVlEN"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BZScZNbROw-N"
      },
      "outputs": [],
      "source": [
        "# TODO?: Automatically upload data to drive such that users who don't already have the data can get it\n",
        "# For now, create a folder in Google Drive and upload the github repository through drive.google.com\n",
        "\n",
        "# TODO: Do we need to support Windows paths?\n",
        "\n",
        "def load_raw_data(raw_data_dir):\n",
        "  # implement this function to load raw data to dataframe/numpy array/tensor\n",
        "  mounted_path_token_embs = raw_data_dir + 'token_embedding_dict.npy'\n",
        "  mounted_path_train = raw_data_dir + 'training.txt'\n",
        "  mounted_path_val = raw_data_dir + 'val.txt'\n",
        "  mounted_path_test = raw_data_dir + 'test.txt'\n",
        "  mounted_path_molecules = raw_data_dir + 'ChEBI_defintions_substructure_corpus.cp'\n",
        "\n",
        "  data_generator = GenerateData(mounted_path_train, mounted_path_val,\n",
        "                                mounted_path_test, mounted_path_molecules,\n",
        "                                mounted_path_token_embs)\n",
        "  transformer_data_generator =  GenerateDataAttention(mounted_path_train,\n",
        "                                                      mounted_path_val,\n",
        "                                                      mounted_path_test,\n",
        "                                                      mounted_path_molecules,\n",
        "                                                      mounted_path_token_embs)\n",
        "\n",
        "  return data_generator, transformer_data_generator\n",
        "\n",
        "def load_processed_data(data_generator, factor_to_reduce_training_set=None):\n",
        "  if factor_to_reduce_training_set is None:\n",
        "    training_set = Dataset(data_generator.generate_examples_train, len(data_generator.training_cids))\n",
        "  else:\n",
        "    reduced_training_cids = data_generator.training_cids[:int(factor_to_reduce_training_set * len(data_generator.training_cids))]\n",
        "    training_set = Dataset(partial(data_generator.generate_examples_custom, reduced_training_cids), len(reduced_training_cids))\n",
        "\n",
        "  validation_set = Dataset(data_generator.generate_examples_val, len(data_generator.validation_cids))\n",
        "  test_set = Dataset(data_generator.generate_examples_test, len(data_generator.test_cids))\n",
        "\n",
        "  return training_set, validation_set, test_set\n",
        "\n",
        "def load_molecule_graph(raw_data_dir, data_generator):\n",
        "  root = 'graph-data/'\n",
        "  graph_data_path = raw_data_dir + \"mol_graphs.zip\"\n",
        "\n",
        "  mg_data_tr = MoleculeGraphDataset(root, data_generator.training_cids, graph_data_path, data_generator)\n",
        "  graph_batcher_tr = CustomGraphCollater(mg_data_tr)\n",
        "  transformer_graph_batcher_tr = CustomGraphCollaterTransformer(mg_data_tr, transformer_data_generator.mol_trunc_length)\n",
        "\n",
        "  mg_data_val = MoleculeGraphDataset(root, data_generator.validation_cids, graph_data_path, data_generator)\n",
        "  graph_batcher_val = CustomGraphCollater(mg_data_val)\n",
        "  transformer_graph_batcher_val = CustomGraphCollaterTransformer(mg_data_val, transformer_data_generator.mol_trunc_length)\n",
        "\n",
        "  mg_data_test = MoleculeGraphDataset(root, data_generator.test_cids, graph_data_path, data_generator)\n",
        "  graph_batcher_test = CustomGraphCollater(mg_data_test)\n",
        "  transformer_graph_batcher_test = CustomGraphCollaterTransformer(mg_data_test, transformer_data_generator.mol_trunc_length)\n",
        "\n",
        "\n",
        "  return mg_data_tr, mg_data_val, mg_data_test, graph_batcher_tr, graph_batcher_val, graph_batcher_test, transformer_graph_batcher_tr, transformer_graph_batcher_val, transformer_graph_batcher_test\n",
        "\n",
        "\n",
        "data_generator, transformer_data_generator = load_raw_data(RAW_DATA_DIR)\n",
        "\n",
        "mg_data_tr, mg_data_val, mg_data_test, \\\n",
        "graph_batcher_tr, graph_batcher_val, graph_batcher_test, \\\n",
        "transformer_graph_batcher_tr, transformer_graph_batcher_val, transformer_graph_batcher_test =\\\n",
        "  load_molecule_graph(RAW_DATA_DIR, data_generator)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# calculate statistics\n",
        "def calculate_stats(data_generator):\n",
        "  # implement this function to calculate the statistics\n",
        "  # it is encouraged to print out the results\n",
        "\n",
        "  train_data_for_stats = list(data_generator.generate_examples_train())\n",
        "  val_data_for_stats = list(data_generator.generate_examples_val())\n",
        "  test_data_for_stats = list(data_generator.generate_examples_test())\n",
        "  print(f'Train size: {len(train_data_for_stats)}')\n",
        "  print(f'Validation size: {len(val_data_for_stats)}')\n",
        "  print(f'Test size: {len(test_data_for_stats)}')\n",
        "\n",
        "  all_data_for_stats = train_data_for_stats + val_data_for_stats + test_data_for_stats\n",
        "  print(f'Total number of molecules: {len(all_data_for_stats)}')\n",
        "\n",
        "  description_lengths_chars = [len(data['input']['text']['original_text']) for data in all_data_for_stats]\n",
        "  # print([data['input']['text']['original_text'] for data in all_data_for_stats[0:5]])\n",
        "\n",
        "  fig, (ax1, ax2) = plt.subplots(nrows=2, ncols=1)\n",
        "  ax1.hist(description_lengths_chars, bins=15, linewidth=0.5, edgecolor=\"white\")\n",
        "  ax1.set_xlabel('Number of Characters')\n",
        "  ax1.set_ylabel('Count')\n",
        "  ax1.set_title('Text Length (characters)')\n",
        "\n",
        "  description_lengths_tokens = [len(data['input']['text']['original_text'].split()) for data in all_data_for_stats]\n",
        "  ax2.hist(description_lengths_tokens, bins=15, linewidth=0.5, edgecolor=\"white\")\n",
        "  ax2.set_xlabel('Number of Tokens')\n",
        "  ax2.set_ylabel('Count')\n",
        "  ax2.set_title('Text Length (tokens)')\n",
        "\n",
        "  plt.tight_layout()\n",
        "  plt.show()\n",
        "\n",
        "  return all_data_for_stats\n",
        "\n",
        "all_data_for_stats = calculate_stats(data_generator)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "vjG7Iryon-Ko"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def calculate_molecule_graph_stats(raw_data_dir, data_generator):\n",
        "  root = 'graph-data/'\n",
        "  graph_data_path = raw_data_dir + \"mol_graphs.zip\"\n",
        "\n",
        "  all_cids = data_generator.training_cids + data_generator.validation_cids + \\\n",
        "    data_generator.test_cids\n",
        "  mg_data_tr = MoleculeGraphDataset(root, all_cids, graph_data_path, data_generator)\n",
        "\n",
        "  num_atoms = []\n",
        "  for i in range(mg_data_tr.len()):\n",
        "    data_entry = mg_data_tr.get(i)\n",
        "    num_atoms.append(data_entry.num_nodes)\n",
        "\n",
        "  fig, ax1 = plt.subplots(nrows=1, ncols=1)\n",
        "  ax1.hist(num_atoms, bins=60, linewidth=0.5, edgecolor=\"white\")\n",
        "  ax1.set_xlabel('Number')\n",
        "  ax1.set_ylabel('Count')\n",
        "  ax1.set_title('Number of Atoms per Molecule')\n",
        "\n",
        "  plt.show()\n",
        "\n",
        "calculate_molecule_graph_stats(RAW_DATA_DIR, data_generator)"
      ],
      "metadata": {
        "id": "HsE6aXHuR9mR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Below is an example data entry parsed from the various data input files.  Each entry represents a molecule along with its associated text description.  entry['cid'] is the Pubchem Compound ID.  entry['input']['text'] is the output of SciBert applied to the text description of the molecule.  entry['input']['molecule'] contains the mol2vec representation of the molecule."
      ],
      "metadata": {
        "id": "iY1AA3PGz5-S"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "entry = all_data_for_stats[0]\n",
        "entry"
      ],
      "metadata": {
        "id": "VeY_FHCb4UQe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Data Loaders for Models**"
      ],
      "metadata": {
        "id": "n9sLIDfL5W-2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "params = {'batch_size': data_generator.batch_size,\n",
        "          'shuffle': True,\n",
        "          'num_workers': 1}\n",
        "\n",
        "factor_to_reduce_training_set = 0.10\n",
        "print(f'Reducing training set by a factor of {factor_to_reduce_training_set}')\n",
        "\n",
        "training_set_for_mlp, validation_set_for_mlp, test_set_for_mlp = load_processed_data(data_generator, factor_to_reduce_training_set)\n",
        "\n",
        "training_generator_for_mlp = DataLoader(training_set_for_mlp, **params)\n",
        "validation_generator_for_mlp = DataLoader(validation_set_for_mlp, **params)\n",
        "test_generator_for_mlp = DataLoader(test_set_for_mlp, **params)\n",
        "\n",
        "training_set_for_gcn, validation_set_for_gcn, test_set_for_gcn = load_processed_data(data_generator, factor_to_reduce_training_set)\n",
        "\n",
        "training_generator_for_gcn = DataLoader(training_set_for_gcn, **params)\n",
        "validation_generator_for_gcn = DataLoader(validation_set_for_gcn, **params)\n",
        "test_generator_for_gcn = DataLoader(test_set_for_gcn, **params)\n",
        "\n",
        "training_set_for_attn, validation_set_for_attn, test_set_for_attn = load_processed_data(transformer_data_generator, factor_to_reduce_training_set)\n",
        "\n",
        "training_generator_for_attn = DataLoader(training_set_for_attn, **params)\n",
        "validation_generator_for_attn = DataLoader(validation_set_for_attn, **params)\n",
        "test_generator_for_attn = DataLoader(test_set_for_attn, **params)"
      ],
      "metadata": {
        "id": "f5TFFgxnxYEq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Models\n",
        "\n",
        "There are three main models in this paper. The first is a Multi-Layer Perceptron (MLP) model. The second is a Graph Convolutional Network. The third is a Transformer Decoder. Each of the models has a normal forward component, but also adds a SciBERT **TODO: Add reference** derivative that encodes the text references of the chemical descriptions.\n",
        "\n"
      ],
      "metadata": {
        "id": "3muyDPFPbozY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Original Instructions\n",
        "**TODO:** *Delete me before submitting*\n",
        "\n",
        "The model includes the model definitation which usually is a class, model training, and other necessary parts.\n",
        "  * Model architecture: layer number/size/type, activation function, etc\n",
        "  * Training objectives: loss function, optimizer, weight of each loss term, etc\n",
        "  * Others: whether the model is pretrained, Monte Carlo simulation for uncertainty analysis, etc\n",
        "  * The code of model should have classes of the model, functions of model training, model validation, etc.\n",
        "  * If your model training is done outside of this notebook, please upload the trained model here and develop a function to load and test it.\n"
      ],
      "metadata": {
        "id": "DpD9TkudWd5S"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Molecular Encoder"
      ],
      "metadata": {
        "id": "eZoazi1BGwI6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Multi-level Perceptron\n",
        "\n",
        "The MLP model is a linear sequence of 3x linear layers using RELU and a normalization layer. That is, it looks like:\n",
        "```\n",
        "        self.molecule_encoder = nn.Sequential(\n",
        "            nn.Linear(nout, nhid),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(nhid, nhid),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(nhid, nout),\n",
        "            nn.LayerNorm(nout)\n",
        "        )\n",
        "```\n",
        "Where the number of inputs is 768, outputs are 300 and size of the hidden layers is 600.\n",
        "\n",
        "The model also contains a BertModel that was pretrained by as 'allenai/scibert_scivocab_uncased'"
      ],
      "metadata": {
        "id": "EYxoFG17G0Kx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class MLPModel(nn.Module):\n",
        "    def __init__(self, ntoken, ninp, nout, nhid, dropout=0.5):\n",
        "        super(MLPModel, self).__init__()\n",
        "\n",
        "        self.text_hidden1 = nn.Linear(ninp, nout)\n",
        "\n",
        "        self.ninp = ninp\n",
        "        self.nhid = nhid\n",
        "        self.nout = nout\n",
        "\n",
        "        self.drop = nn.Dropout(p=dropout)\n",
        "\n",
        "        self.mol_hidden1 = nn.Linear(nout, nhid)\n",
        "        self.mol_hidden2 = nn.Linear(nhid, nhid)\n",
        "        self.mol_hidden3 = nn.Linear(nhid, nout)\n",
        "\n",
        "\n",
        "        self.temp = nn.Parameter(torch.Tensor([0.07]))\n",
        "        self.register_parameter( 'temp' , self.temp )\n",
        "\n",
        "        self.ln1 = nn.LayerNorm((nout))\n",
        "        self.ln2 = nn.LayerNorm((nout))\n",
        "\n",
        "        self.relu = nn.ReLU()\n",
        "        self.selu = nn.SELU()\n",
        "\n",
        "        self.other_params = list(self.parameters()) #get all but bert params\n",
        "\n",
        "        self.text_transformer_model = BertModel.from_pretrained('allenai/scibert_scivocab_uncased')\n",
        "        self.text_transformer_model.train()\n",
        "\n",
        "    def forward(self, text, molecule, text_mask = None, molecule_mask = None):\n",
        "\n",
        "        text_encoder_output = self.text_transformer_model(text,\n",
        "                                                          attention_mask = text_mask)\n",
        "\n",
        "        text_x = text_encoder_output['pooler_output']\n",
        "        text_x = self.text_hidden1(text_x)\n",
        "\n",
        "        x = self.relu(self.mol_hidden1(molecule))\n",
        "        x = self.relu(self.mol_hidden2(x))\n",
        "        x = self.mol_hidden3(x)\n",
        "\n",
        "\n",
        "        x = self.ln1(x)\n",
        "        text_x = self.ln2(text_x)\n",
        "\n",
        "        x = x * torch.exp(self.temp)\n",
        "        text_x = text_x * torch.exp(self.temp)\n",
        "\n",
        "        return text_x, x\n",
        "\n",
        "mlp_model = MLPModel(ntoken = data_generator.text_tokenizer.vocab_size, ninp = 768, nhid = 600,\n",
        "                 nout = 300)"
      ],
      "metadata": {
        "id": "Sh1-GS5ZG2mK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(DEVICE)\n",
        "\n",
        "tmp = mlp_model.to(DEVICE)"
      ],
      "metadata": {
        "id": "TnOV5hWcV5wh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "mounted_path = \"MLP_output/\"\n",
        "if not os.path.exists(mounted_path):\n",
        "  os.mkdir(mounted_path)\n",
        "\n",
        "import torch.optim as optim\n",
        "from transformers.optimization import get_linear_schedule_with_warmup\n",
        "\n",
        "epochs = NUM_EPOCHS\n",
        "\n",
        "init_lr = 1e-4\n",
        "bert_lr = 3e-5\n",
        "bert_params = list(mlp_model.text_transformer_model.parameters())\n",
        "\n",
        "optimizer = optim.Adam([\n",
        "                {'params': mlp_model.other_params},\n",
        "                {'params': bert_params, 'lr': bert_lr}\n",
        "            ], lr=init_lr)\n",
        "\n",
        "num_warmup_steps = 1000\n",
        "num_training_steps = epochs * len(training_generator_for_mlp) - num_warmup_steps\n",
        "scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps = num_warmup_steps, num_training_steps = num_training_steps)\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "def loss_func_mlp(v1, v2):\n",
        "  logits = torch.matmul(v1,torch.transpose(v2, 0, 1))\n",
        "  labels = torch.arange(logits.shape[0]).to(DEVICE)\n",
        "  return criterion(logits, labels) + criterion(torch.transpose(logits, 0, 1), labels)\n",
        "\n",
        "train_losses = []\n",
        "val_losses = []\n",
        "\n",
        "train_acc = []\n",
        "val_acc = []\n",
        "\n",
        "def train_mlp_model_one_iter(epoch, model, loss_func_mlp, optimizer, scheduler):\n",
        "  # Training\n",
        "\n",
        "  start_time = time.time()\n",
        "  running_loss = 0.0\n",
        "  running_acc = 0.0\n",
        "  model.train()\n",
        "  for i, d in enumerate(training_generator_for_mlp):\n",
        "    batch, labels = d\n",
        "    # Transfer to GPU\n",
        "\n",
        "    text_mask = batch['text']['attention_mask'].bool()\n",
        "\n",
        "    text = batch['text']['input_ids'].to(DEVICE)\n",
        "    text_mask = text_mask.to(DEVICE)\n",
        "    molecule = batch['molecule']['mol2vec'].float().to(DEVICE)\n",
        "\n",
        "    text_out, chem_out = model(text, molecule, text_mask)\n",
        "\n",
        "    loss = loss_func_mlp(text_out, chem_out).to(DEVICE)\n",
        "\n",
        "    running_loss += loss.item()\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    scheduler.step()\n",
        "\n",
        "    if (i+1) % 100 == 0:\n",
        "      print(i+1, \"batches trained. Avg loss:\\t\", running_loss / (i+1), \". Avg ms/step =\", 1000*(time.time()-start_time)/(i+1))\n",
        "  train_losses.append(running_loss / (i+1))\n",
        "  train_acc.append(running_acc / (i+1))\n",
        "\n",
        "  print(\"Epoch\", epoch, \"training loss:\\t\\t\", running_loss / (i+1), \". Time =\", (time.time()-start_time), \"seconds.\")\n",
        "\n",
        "  # Validation\n",
        "  model.eval()\n",
        "  with torch.set_grad_enabled(False):\n",
        "    start_time = time.time()\n",
        "    running_acc = 0.0\n",
        "    running_loss = 0.0\n",
        "    for i, d in enumerate(validation_generator_for_mlp):\n",
        "      batch, labels = d\n",
        "        # Transfer to GPU\n",
        "\n",
        "      text_mask = batch['text']['attention_mask'].bool()\n",
        "\n",
        "      text = batch['text']['input_ids'].to(DEVICE)\n",
        "      text_mask = text_mask.to(DEVICE)\n",
        "      molecule = batch['molecule']['mol2vec'].float().to(DEVICE)\n",
        "\n",
        "      text_out, chem_out = model(text, molecule, text_mask)\n",
        "\n",
        "      loss = loss_func_mlp(text_out, chem_out).to(DEVICE)\n",
        "      running_loss += loss.item()\n",
        "\n",
        "      if (i+1) % 100 == 0:\n",
        "        print(i+1, \"batches eval. Avg loss:\\t\", running_loss / (i+1), \". Avg ms/step =\", 1000*(time.time()-start_time)/(i+1))\n",
        "    val_losses.append(running_loss / (i+1))\n",
        "    val_acc.append(running_acc / (i+1))\n",
        "\n",
        "    min_loss = np.min(val_losses)\n",
        "    if val_losses[-1] == min_loss:\n",
        "      torch.save(model.state_dict(), mounted_path + 'weights_pretrained.{epoch:02d}-{min_loss:.2f}.pt'.format(epoch = epoch, min_loss = min_loss))\n",
        "\n",
        "    print(\"Epoch\", epoch, \"validation loss:\\t\", running_loss / (i+1), \". Time =\", (time.time()-start_time), \"seconds.\")\n",
        "    #print(\"Train Loss: %.2f, Validation Loss: %.2f\" % (train_loss, valid_loss))\n",
        "\n",
        "# Boolean flag above to decide whether to run training\n",
        "if TRAIN_MLP:\n",
        "  for i in range(epochs):\n",
        "    train_mlp_model_one_iter(i, mlp_model, loss_func_mlp, optimizer, scheduler)\n",
        "\n",
        "  torch.save(mlp_model.state_dict(), mounted_path + \"final_weights.\"+str(epochs)+\".pt\")"
      ],
      "metadata": {
        "id": "d6lDrOb2Onm0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The MLP model in the paper was trained for 40 epochs. On a NVIDIA V100 it takes ~330s to train one epoch. For brevity, we instead load the model here.\n",
        "\n",
        "On an A100 system, the MLP model takes about 4 hours to train at the cost of ~$5."
      ],
      "metadata": {
        "id": "-9VU8x6wMYYl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Graph Convolutional Network\n",
        "\n",
        "The GCN model in Text2Mol has 3 parts:\n",
        "*   Text processing transformer model from SciBERT that is identical to the MLP above.\n",
        "*   Graph Convulational Network (GCN), and\n",
        "*   Graph readout layers\n"
      ],
      "metadata": {
        "id": "eW1hpG0BG5av"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class GCNModel(nn.Module):\n",
        "    def __init__(self, ntoken, ninp, nout, nhid, graph_hidden_channels, dropout=0.5):\n",
        "        super(GCNModel, self).__init__()\n",
        "\n",
        "\n",
        "        self.text_hidden1 = nn.Linear(ninp, nout)\n",
        "\n",
        "        self.ninp = ninp\n",
        "        self.nhid = nhid\n",
        "        self.nout = nout\n",
        "\n",
        "        self.drop = nn.Dropout(p=dropout)\n",
        "\n",
        "        self.temp = nn.Parameter(torch.Tensor([0.07]))\n",
        "        self.register_parameter( 'temp' , self.temp )\n",
        "\n",
        "        self.ln1 = nn.LayerNorm((nout))\n",
        "        self.ln2 = nn.LayerNorm((nout))\n",
        "\n",
        "        self.relu = nn.ReLU()\n",
        "        self.selu = nn.SELU()\n",
        "\n",
        "        #For GCN:\n",
        "        self.conv1 = GCNConv(mg_data_val.num_node_features, graph_hidden_channels)\n",
        "        self.conv2 = GCNConv(graph_hidden_channels, graph_hidden_channels)\n",
        "        self.conv3 = GCNConv(graph_hidden_channels, graph_hidden_channels)\n",
        "        self.mol_hidden1 = nn.Linear(graph_hidden_channels, nhid)\n",
        "        self.mol_hidden2 = nn.Linear(nhid, nhid)\n",
        "        self.mol_hidden3 = nn.Linear(nhid, nout)\n",
        "\n",
        "\n",
        "        self.other_params = list(self.parameters()) #get all but bert params\n",
        "\n",
        "        self.text_transformer_model = BertModel.from_pretrained('allenai/scibert_scivocab_uncased')\n",
        "        self.text_transformer_model.train()\n",
        "\n",
        "    def forward(self, text, graph_batch, text_mask = None, molecule_mask = None):\n",
        "\n",
        "        text_encoder_output = self.text_transformer_model(text, attention_mask = text_mask)\n",
        "\n",
        "        text_x = text_encoder_output['pooler_output']\n",
        "        text_x = self.text_hidden1(text_x)\n",
        "\n",
        "\n",
        "        #Obtain node embeddings\n",
        "        x = graph_batch.x\n",
        "        edge_index = graph_batch.edge_index\n",
        "        batch = graph_batch.batch\n",
        "        x = self.conv1(x, edge_index)\n",
        "        x = x.relu()\n",
        "        x = self.conv2(x, edge_index)\n",
        "        x = x.relu()\n",
        "        x = self.conv3(x, edge_index)\n",
        "\n",
        "        # Readout layer\n",
        "        x = global_mean_pool(x, batch)  # [batch_size, graph_hidden_channels]\n",
        "\n",
        "\n",
        "        x = self.mol_hidden1(x).relu()\n",
        "        x = self.mol_hidden2(x).relu()\n",
        "        x = self.mol_hidden3(x)\n",
        "\n",
        "\n",
        "        x = self.ln1(x)\n",
        "        text_x = self.ln2(text_x)\n",
        "\n",
        "        x = x * torch.exp(self.temp)\n",
        "        text_x = text_x * torch.exp(self.temp)\n",
        "\n",
        "        return text_x, x\n",
        "\n",
        "gcn_model = GCNModel(ntoken = data_generator.text_tokenizer.vocab_size, ninp = 768, nhid = 600,\n",
        "              nout = 300, graph_hidden_channels = 600)"
      ],
      "metadata": {
        "id": "DhJS14SjG8Z5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(DEVICE)\n",
        "\n",
        "tmp = gcn_model.to(DEVICE)"
      ],
      "metadata": {
        "id": "SJSTTeyyGrv5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "epochs = NUM_EPOCHS\n",
        "\n",
        "init_lr = 1e-4\n",
        "bert_lr = 3e-5\n",
        "bert_params = list(gcn_model.text_transformer_model.parameters())\n",
        "\n",
        "optimizer = optim.Adam([\n",
        "                {'params': gcn_model.other_params},\n",
        "                {'params': bert_params, 'lr': bert_lr}\n",
        "            ], lr=init_lr)\n",
        "\n",
        "num_warmup_steps = 1000\n",
        "num_training_steps = epochs * len(training_generator_for_gcn) - num_warmup_steps\n",
        "scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps = num_warmup_steps, num_training_steps = num_training_steps)\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "def loss_func_gcn(v1, v2):\n",
        "  logits = torch.matmul(v1,torch.transpose(v2, 0, 1))\n",
        "  labels = torch.arange(logits.shape[0]).to(DEVICE)\n",
        "  return criterion(logits, labels) + criterion(torch.transpose(logits, 0, 1), labels)\n",
        "\n",
        "train_losses = []\n",
        "val_losses = []\n",
        "\n",
        "train_acc = []\n",
        "val_acc = []\n",
        "\n",
        "mounted_path = \"GCN_output/\"\n",
        "if not os.path.exists(mounted_path):\n",
        "  os.makedirs(mounted_path)\n",
        "\n",
        "# Loop over epochs\n",
        "def train_gcn_model_one_iter(epoch, model, loss_func_gcn, optimizer, scheduler):\n",
        "    # Training\n",
        "\n",
        "    start_time = time.time()\n",
        "    running_loss = 0.0\n",
        "    running_acc = 0.0\n",
        "    model.train()\n",
        "    for i, d in enumerate(training_generator_for_gcn):\n",
        "        batch, labels = d\n",
        "        # Transfer to GPU\n",
        "\n",
        "        text_mask = batch['text']['attention_mask'].bool()\n",
        "\n",
        "        text = batch['text']['input_ids'].to(DEVICE)\n",
        "        text_mask = text_mask.to(DEVICE)\n",
        "        graph_batch = graph_batcher_tr(d[0]['molecule']['cid']).to(DEVICE)\n",
        "\n",
        "\n",
        "        text_out, chem_out = model(text, graph_batch, text_mask)\n",
        "\n",
        "        loss = loss_func_gcn(text_out, chem_out).to(DEVICE)\n",
        "\n",
        "        running_loss += loss.item()\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        scheduler.step()\n",
        "\n",
        "        if (i+1) % 100 == 0:\n",
        "          print(i+1, \"batches trained. Avg loss:\\t\", running_loss / (i+1), \". Avg ms/step =\", 1000*(time.time()-start_time)/(i+1))\n",
        "    train_losses.append(running_loss / (i+1))\n",
        "    train_acc.append(running_acc / (i+1))\n",
        "\n",
        "    print(\"Epoch\", epoch, \"training loss:\\t\\t\", running_loss / (i+1), \". Time =\", (time.time()-start_time), \"seconds.\")\n",
        "\n",
        "    # Validation\n",
        "    gcn_model.eval()\n",
        "    with torch.set_grad_enabled(False):\n",
        "      start_time = time.time()\n",
        "      running_acc = 0.0\n",
        "      running_loss = 0.0\n",
        "      for i, d in enumerate(validation_generator_for_gcn):\n",
        "          batch, labels = d\n",
        "          # Transfer to GPU\n",
        "\n",
        "          text_mask = batch['text']['attention_mask'].bool()\n",
        "\n",
        "          text = batch['text']['input_ids'].to(DEVICE)\n",
        "          text_mask = text_mask.to(DEVICE)\n",
        "          graph_batch = graph_batcher_val(d[0]['molecule']['cid']).to(DEVICE)\n",
        "\n",
        "\n",
        "          text_out, chem_out = gcn_model(text, graph_batch, text_mask)\n",
        "\n",
        "          loss = loss_func_gcn(text_out, chem_out).to(DEVICE)\n",
        "          running_loss += loss.item()\n",
        "\n",
        "          if (i+1) % 100 == 0:\n",
        "            print(i+1, \"batches eval. Avg loss:\\t\", running_loss / (i+1), \". Avg ms/step =\", 1000*(time.time()-start_time)/(i+1))\n",
        "      val_losses.append(running_loss / (i+1))\n",
        "      val_acc.append(running_acc / (i+1))\n",
        "\n",
        "\n",
        "      min_loss = np.min(val_losses)\n",
        "      if val_losses[-1] == min_loss:\n",
        "          torch.save(gcn_model.state_dict(), mounted_path + 'weights_pretrained.{epoch:02d}-{min_loss:.2f}.pt'.format(epoch = epoch, min_loss = min_loss))\n",
        "\n",
        "    print(\"Epoch\", epoch, \"validation loss:\\t\", running_loss / (i+1), \". Time =\", (time.time()-start_time), \"seconds.\")\n",
        "\n",
        "# Boolean flag above to decide whether to run training\n",
        "if TRAIN_GCN:\n",
        "  for i in range(epochs):\n",
        "    train_gcn_model_one_iter(i, gcn_model, loss_func_gcn, optimizer, scheduler)\n",
        "\n",
        "  #Save last accuracy:\n",
        "  torch.save(gcn_model.state_dict(), mounted_path + \"final_weights.\"+str(epochs)+\".pt\")\n",
        "\n"
      ],
      "metadata": {
        "id": "TEsVQ2yEEA3K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Transformer Decoder\n",
        "\n",
        "The final part of the paper is a Transformer Decoder that mates between the different parts of the model.\n"
      ],
      "metadata": {
        "id": "01XZegMTHU7A"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class TransformerModel(nn.Module):\n",
        "\n",
        "    def __init__(self, ntoken, ninp, nout, nhid, nhead, nlayers, graph_hidden_channels, mol_trunc_length,  dropout=0.5):\n",
        "        super(TransformerModel, self).__init__()\n",
        "\n",
        "        self.text_hidden1 = nn.Linear(ninp, nhid)\n",
        "        self.text_hidden2 = nn.Linear(nhid, nout)\n",
        "\n",
        "        self.ninp = ninp\n",
        "        self.nhid = nhid\n",
        "        self.nout = nout\n",
        "        self.graph_hidden_channels = graph_hidden_channels\n",
        "\n",
        "        self.drop = nn.Dropout(p=dropout)\n",
        "\n",
        "        decoder_layers = TransformerDecoderLayer(ninp, nhead, nhid, dropout)\n",
        "        self.text_transformer_decoder = TransformerDecoder(decoder_layers, nlayers)\n",
        "\n",
        "\n",
        "        self.temp = nn.Parameter(torch.Tensor([0.07]))\n",
        "        self.register_parameter( 'temp' , self.temp )\n",
        "\n",
        "        self.ln1 = nn.LayerNorm((nout))\n",
        "        self.ln2 = nn.LayerNorm((nout))\n",
        "\n",
        "        self.relu = nn.ReLU()\n",
        "        self.selu = nn.SELU()\n",
        "\n",
        "        #For GCN:\n",
        "        self.conv1 = GCNConv(mg_data_val.num_node_features, graph_hidden_channels)\n",
        "        self.conv2 = GCNConv(graph_hidden_channels, graph_hidden_channels)\n",
        "        self.conv3 = GCNConv(graph_hidden_channels, graph_hidden_channels)\n",
        "        self.mol_hidden1 = nn.Linear(graph_hidden_channels, nhid)\n",
        "        self.mol_hidden2 = nn.Linear(nhid, nout)\n",
        "\n",
        "\n",
        "        self.other_params = list(self.parameters()) #get all but bert params\n",
        "\n",
        "        self.text_transformer_model = BertModel.from_pretrained('allenai/scibert_scivocab_uncased')\n",
        "        self.text_transformer_model.train()\n",
        "\n",
        "        self.device = 'cpu'\n",
        "\n",
        "    def set_device(self, dev):\n",
        "        self.to(dev)\n",
        "        self.device = dev\n",
        "\n",
        "    def forward(self, text, graph_batch, text_mask = None, molecule_mask = None):\n",
        "\n",
        "        text_encoder_output = self.text_transformer_model(text, attention_mask = text_mask)\n",
        "\n",
        "        #Obtain node embeddings\n",
        "        x = graph_batch.x\n",
        "        edge_index = graph_batch.edge_index\n",
        "        batch = graph_batch.batch\n",
        "        x = self.conv1(x, edge_index)\n",
        "        x = x.relu()\n",
        "        x = self.conv2(x, edge_index)\n",
        "        x = x.relu()\n",
        "        mol_x = self.conv3(x, edge_index)\n",
        "\n",
        "        #turn pytorch geometric output into the correct format for transformer\n",
        "        #requires recovering the nodes from each graph into a separate dimension\n",
        "        node_features = torch.zeros((graph_batch.num_graphs,\n",
        "                                     transformer_data_generator.mol_trunc_length,\n",
        "                                     self.graph_hidden_channels)).to(self.device)\n",
        "        for i, p in enumerate(graph_batch.ptr):\n",
        "          if p == 0:\n",
        "            old_p = p\n",
        "            continue\n",
        "          node_features[i - 1, :p-old_p, :] = mol_x[old_p:torch.min(p, old_p + transformer_data_generator.mol_trunc_length), :]\n",
        "          old_p = p\n",
        "        node_features = torch.transpose(node_features, 0, 1)\n",
        "\n",
        "        text_output = self.text_transformer_decoder(text_encoder_output['last_hidden_state'].transpose(0,1), node_features,\n",
        "                                                            tgt_key_padding_mask = text_mask == 0, memory_key_padding_mask = ~molecule_mask)\n",
        "\n",
        "\n",
        "        #Readout layer\n",
        "        x = global_mean_pool(mol_x, batch)  # [batch_size, graph_hidden_channels]\n",
        "\n",
        "        x = self.mol_hidden1(x)\n",
        "        x = x.relu()\n",
        "        x = self.mol_hidden2(x)\n",
        "\n",
        "        text_x = torch.tanh(self.text_hidden1(text_output[0,:,:])) #[CLS] pooler\n",
        "        text_x = self.text_hidden2(text_x)\n",
        "\n",
        "        x = self.ln1(x)\n",
        "        text_x = self.ln2(text_x)\n",
        "\n",
        "        x = x * torch.exp(self.temp)\n",
        "        text_x = text_x * torch.exp(self.temp)\n",
        "\n",
        "        return text_x, x\n"
      ],
      "metadata": {
        "id": "Cf5qjmgv11n-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "transformer_model = TransformerModel(ntoken = transformer_data_generator.text_tokenizer.vocab_size,\n",
        "                                     ninp = 768,\n",
        "                                     nout = 300, mol_trunc_length = 512, nhead = 8,\n",
        "                                     nhid = 512, nlayers = 3, graph_hidden_channels = 768)\n",
        "\n",
        "print(DEVICE)\n",
        "tmp = transformer_model.set_device(DEVICE)"
      ],
      "metadata": {
        "id": "CsUnd15U2KCP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "transformer_training_set = Dataset(transformer_data_generator.generate_examples_train,\n",
        "                                   len(transformer_data_generator.training_cids))\n",
        "transformer_validation_set = Dataset(transformer_data_generator.generate_examples_val,\n",
        "                                     len(transformer_data_generator.validation_cids))\n",
        "transformer_test_set = Dataset(transformer_data_generator.generate_examples_test,\n",
        "                               len(transformer_data_generator.test_cids))"
      ],
      "metadata": {
        "id": "56DWkZtuSCSz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Parameters\n",
        "params = {'batch_size': transformer_data_generator.batch_size,\n",
        "          'shuffle': True,\n",
        "          'num_workers': 1}\n",
        "\n",
        "transformer_training_generator = DataLoader(transformer_training_set, **params)\n",
        "transformer_validation_generator = DataLoader(transformer_validation_set, **params)\n",
        "transformer_test_generator = DataLoader(transformer_test_set, **params)"
      ],
      "metadata": {
        "id": "GFUYw2suSoNY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.optim as optim\n",
        "from transformers.optimization import get_linear_schedule_with_warmup\n",
        "\n",
        "epochs = NUM_EPOCHS\n",
        "init_lr = 1e-4\n",
        "bert_lr = 3e-5\n",
        "bert_params = list(transformer_model.text_transformer_model.parameters())\n",
        "\n",
        "optimizer = optim.Adam([\n",
        "                {'params': transformer_model.other_params},\n",
        "                {'params': bert_params, 'lr': bert_lr}\n",
        "            ], lr=init_lr)\n",
        "\n",
        "num_warmup_steps = 1000\n",
        "num_training_steps = epochs * len(transformer_training_generator) - num_warmup_steps\n",
        "scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps = num_warmup_steps,\n",
        "                                            num_training_steps = num_training_steps)"
      ],
      "metadata": {
        "id": "UBeFEjOsTHLa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "mounted_path = \"transformer-output/\"\n",
        "if not os.path.exists(mounted_path):\n",
        "  os.mkdir(mounted_path)\n",
        "\n",
        "criterion = nn.BCEWithLogitsLoss()\n",
        "\n",
        "def loss_func_transformer(v1, v2, labels):\n",
        "  logits = torch.matmul(v1,torch.transpose(v2, 0, 1))\n",
        "  eye = torch.diag_embed(labels).to(DEVICE)\n",
        "  return criterion(logits, eye) + criterion(torch.transpose(logits, 0, 1), eye), \\\n",
        "          logits.diag() > 0\n",
        "\n",
        "\n",
        "# Loop over epochs\n",
        "def train_transformer_model_one_iter(epoch, model, loss_func, optimizer, scheduler):\n",
        "    # Training\n",
        "\n",
        "    start_time = time.time()\n",
        "    running_loss = 0.0\n",
        "    running_acc = 0.0\n",
        "    model.train()\n",
        "    for i, d in enumerate(transformer_training_generator):\n",
        "        batch, labels = d\n",
        "        # Transfer to GPU\n",
        "        text = batch['text']['input_ids'].to(DEVICE)\n",
        "        text_mask = batch['text']['attention_mask'].bool().to(DEVICE)\n",
        "        graph_batch, molecule_mask = transformer_graph_batcher_tr(d[0]['molecule']['cid'])\n",
        "        graph_batch = graph_batch.to(DEVICE)\n",
        "        molecule_mask = molecule_mask.to(DEVICE)\n",
        "\n",
        "        labels = labels.float().to(DEVICE)\n",
        "\n",
        "        text_out, chem_out = model(text, graph_batch, text_mask, molecule_mask)\n",
        "        loss, pred = loss_func(text_out, chem_out, labels)\n",
        "        if torch.isnan(loss): zz\n",
        "\n",
        "        running_loss += loss.item()\n",
        "        running_acc += np.sum((pred.squeeze().cpu().detach().numpy() > 0) ==\\\n",
        "                              labels.cpu().detach().numpy()) / labels.shape[0]\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        scheduler.step()\n",
        "\n",
        "        if (i+1) % 100 == 0:\n",
        "          print(i+1, \"batches trained. Avg loss:\\t\", running_loss / (i+1), \\\n",
        "                \"Acc:\", str(running_acc / (i+1)), \". Avg ms/step =\", \\\n",
        "                1000*(time.time()-start_time)/(i+1))\n",
        "\n",
        "    train_losses.append(running_loss / (i+1))\n",
        "    train_acc.append(running_acc / (i+1))\n",
        "\n",
        "    print(\"Epoch\", epoch, \"training loss:\\t\\t\", running_loss / (i+1),\n",
        "          \". Time =\", (time.time()-start_time), \"seconds.\")\n",
        "    print(\"Training accuracy:\", train_acc[-1])\n",
        "\n",
        "    # Validation\n",
        "    model.eval()\n",
        "    with torch.set_grad_enabled(False):\n",
        "      start_time = time.time()\n",
        "      running_acc = 0.0\n",
        "      running_loss = 0.0\n",
        "      for i, d in enumerate(transformer_training_generator):\n",
        "          batch, labels = d\n",
        "          # Transfer to GPU\n",
        "\n",
        "          text = batch['text']['input_ids'].to(device)\n",
        "          text_mask = batch['text']['attention_mask'].bool().to(device)\n",
        "          graph_batch, molecule_mask = transformer_graph_batcher_val(d[0]['molecule']['cid'])\n",
        "          graph_batch = graph_batch.to(device)\n",
        "          molecule_mask = molecule_mask.to(device)\n",
        "\n",
        "          labels = labels.float().to(device)\n",
        "\n",
        "          text_out, chem_out = model(text, graph_batch, text_mask, molecule_mask)\n",
        "          loss, pred = loss_func(text_out, chem_out, labels)\n",
        "\n",
        "          running_loss += loss.item()\n",
        "          running_acc += np.sum((pred.squeeze().cpu().detach().numpy() > 0) ==\\\n",
        "                                labels.cpu().detach().numpy()) / labels.shape[0]\n",
        "\n",
        "          if (i+1) % 100 == 0:\n",
        "            print(i+1, \"batches eval. Avg loss:\\t\", running_loss / (i+1),\n",
        "                  \". Avg ms/step =\", 1000*(time.time()-start_time)/(i+1))\n",
        "      val_losses.append(running_loss / (i+1))\n",
        "      val_acc.append(running_acc / (i+1))\n",
        "\n",
        "\n",
        "      min_loss = np.min(val_losses)\n",
        "      if val_losses[-1] == min_loss:\n",
        "          torch.save(gcn_model.state_dict(), mounted_path + \\\n",
        "                     'weights_pretrained.{epoch:02d}-{min_loss:.2f}.pt'.format(epoch = epoch, min_loss = min_loss))\n",
        "\n",
        "    print(\"Epoch\", epoch, \"validation loss:\\t\", running_loss / (i+1),\n",
        "          \". Time =\", (time.time()-start_time), \"seconds.\")\n",
        "    print(\"Validation accuracy:\", val_acc[-1])\n",
        "\n",
        "# Boolean flag above to decide whether to run training\n",
        "if TRAIN_TRANSFORMER:\n",
        "  for i in range(epochs):\n",
        "    train_transformer_model_one_iter(i, transformer_model, loss_func_transformer,\n",
        "                                     optimizer, scheduler)\n",
        "\n",
        "  torch.save(model.state_dict(), mounted_path + \"final_weights.\"+str(epochs)+\".pt\")"
      ],
      "metadata": {
        "id": "pcvYDTSlTgS-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Results\n",
        "In this section, you should finish training your model training or loading your trained model. That is a great experiment! You should share the results with others with necessary metrics and figures.\n",
        "\n",
        "Please test and report results for all experiments that you run with:\n",
        "\n",
        "*   specific numbers (accuracy, AUC, RMSE, etc)\n",
        "*   figures (loss shrinkage, outputs from GAN, annotation or label of sample pictures, etc)\n"
      ],
      "metadata": {
        "id": "gX6bCcZNuxmz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "gcn_model.load_state_dict(torch.load(GCN_MODEL_PATH))\n",
        "tmp = gcn_model.eval()\n",
        "\n",
        "mlp_model.load_state_dict(torch.load(MLP_MODEL_PATH))\n",
        "tmp = mlp_model.eval()\n",
        "\n",
        "transformer_model.load_state_dict(torch.load(TRANSFORMER_MODEL_PATH))\n",
        "tmp = transformer_model.eval()"
      ],
      "metadata": {
        "id": "qIlkWzgoeWI4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# metrics to evaluate my model\n",
        "\n",
        "# plot figures to better show the results\n",
        "\n",
        "# it is better to save the numbers and figures for your presentation."
      ],
      "metadata": {
        "id": "LjW9bCkouv8O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Code to generate rank metrics based on the embeddings (for brevity only shown for training embeddings)\n",
        "# Metrics displayed - Mean rank, Hits at 1 / 10 / 100 / 500 / 1000, MRR\n",
        "\n",
        "# Fix the directory where the embeddings are stored, if it is a different directory, update this variable\n",
        "dir = \"MLP_embeddings/embeddings/\"\n",
        "\n",
        "# Do similar for test and validation and load the appropriate .npy files\n",
        "cids_train = np.load(osp.join(dir, \"cids_train.npy\"), allow_pickle=True)\n",
        "text_embeddings_train = np.load(osp.join(dir, \"text_embeddings_train.npy\"))\n",
        "chem_embeddings_train = np.load(osp.join(dir, \"chem_embeddings_train.npy\"))\n",
        "\n",
        "# If want to combine validation and test, add it to np.concatenate for all_text_embeddings\n",
        "# and all_mol_embeddings\n",
        "all_mol_embeddings = np.concatenate(chem_embeddings_train, axis = 0)\n",
        "\n",
        "all_cids = np.concatenate(cids_train, axis = 0)\n",
        "\n",
        "n_train = len(cids_train)\n",
        "# If have test and validation get the length and add it and store in n\n",
        "# E.x. n_test = len(cids_test)\n",
        "#      n = n_train + n_test\n",
        "# Do similar for validation\n",
        "n = n_train\n",
        "\n",
        "def memory_efficient_similarity_matrix_custom(func, embedding1, embedding2, chunk_size = 1000):\n",
        "    rows = embedding1.shape[0]\n",
        "\n",
        "    num_chunks = int(np.ceil(rows / chunk_size))\n",
        "\n",
        "    for i in range(num_chunks):\n",
        "        end_chunk = (i+1)*(chunk_size) if (i+1)*(chunk_size) < rows else rows #account for smaller chunk at end...\n",
        "        yield func(embedding1[i*chunk_size:end_chunk,:], embedding2)\n",
        "\n",
        "text_chem_cos = memory_efficient_similarity_matrix_custom(cosine_similarity, text_embeddings_train, all_mol_embeddings)\n",
        "\n",
        "# Calculate Ranks:\n",
        "tr_avg_ranks = np.zeros((n_train, n))\n",
        "ranks_train = []\n",
        "\n",
        "def get_ranks(text_chem_cos, ranks_avg, offset, split= \"\"):\n",
        "    ranks_tmp = []\n",
        "    j = 0 #keep track of all loops\n",
        "    for l, emb in enumerate(text_chem_cos):\n",
        "        for k in range(emb.shape[0]):\n",
        "            cid_locs = np.argsort(emb[k,:])[::-1]\n",
        "            ranks = np.argsort(cid_locs)\n",
        "\n",
        "            ranks_avg[j,:] = ranks_avg[j,:] + ranks\n",
        "\n",
        "            rank = ranks[j+offset] + 1\n",
        "            ranks_tmp.append(rank)\n",
        "\n",
        "\n",
        "            j += 1\n",
        "            if j % 1000 == 0: print(j, split+\" processed\")\n",
        "\n",
        "    return np.array(ranks_tmp)\n",
        "\n",
        "# Output will look like the following\n",
        "# For train\n",
        "# Train Model:\n",
        "# Mean rank : ....\n",
        "# Hits at 1 : ....\n",
        "# Hits at 10: ....\n",
        "# ......\n",
        "def print_ranks(ranks, split):\n",
        "\n",
        "    print(split+\" Model:\")\n",
        "    print(\"Mean rank:\", np.mean(ranks))\n",
        "    print(\"Hits at 1:\", np.mean(ranks <= 1))\n",
        "    print(\"Hits at 10:\", np.mean(ranks <= 10))\n",
        "    print(\"Hits at 100:\", np.mean(ranks <= 100))\n",
        "    print(\"Hits at 500:\", np.mean(ranks <= 500))\n",
        "    print(\"Hits at 1000:\", np.mean(ranks <= 1000))\n",
        "\n",
        "    print(\"MRR:\", np.mean(1/ranks))\n",
        "    print()\n",
        "\n",
        "# If doing for test and/or validation remember to set correct offset.\n",
        "ranks_tmp = get_ranks(text_chem_cos, tr_avg_ranks, offset=0, split=\"train\")\n",
        "print_ranks(ranks_tmp, split=\"Training\")\n",
        "ranks_train = ranks_tmp"
      ],
      "metadata": {
        "id": "0PuHENFVbDBT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "For the code above, here is example output of the MLP model being trained for 2 epochs with a batch size of 32\n",
        "\n",
        "Training Model: <br/>\n",
        "Mean rank: 353.561042108452 <br/>\n",
        "Hits at 1: 0.010413511057255378 <br/>\n",
        "Hits at 10: 0.09133595880036352 <br/>\n",
        "Hits at 100: 0.4669797031202666 <br/>\n",
        "Hits at 500: 0.8363753408058164 <br/>\n",
        "Hits at 1000: 0.9269539533474704 <br/>\n",
        "MRR: 0.04210451719083336 <br/>"
      ],
      "metadata": {
        "id": "-CZKCrYdHQj0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# To plot cosine score vs ranking using embeddings\n",
        "# For brevity, only showing for \"train\". The process for validation and test is similar.\n",
        "\n",
        "dir = \"MLP_embeddings/embeddings/\"\n",
        "output_file = \"output_file.png\"\n",
        "\n",
        "# For validation and test load it similar to train.\n",
        "cids_train = np.load(osp.join(dir, \"cids_train.npy\"), allow_pickle=True)\n",
        "text_embeddings_train = np.load(osp.join(dir, \"text_embeddings_train.npy\"))\n",
        "chem_embeddings_train = np.load(osp.join(dir, \"chem_embeddings_train.npy\"))\n",
        "\n",
        "# Do similar for validation and test if wanted and add it into np.concatenate\n",
        "all_text_embbedings = np.concatenate(text_embeddings_train, axis = 0)\n",
        "all_mol_embeddings = np.concatenate(chem_embeddings_train, axis = 0)\n",
        "\n",
        "#all_cids = np.concatenate(cids_train, axis = 0)\n",
        "all_cids = cids_train\n",
        "\n",
        "n_train = len(cids_train)\n",
        "# If you have cids_val and cids_test make sure to add it to n below.\n",
        "n = n_train\n",
        "\n",
        "offset_val = n_train\n",
        "\n",
        "def memory_efficient_similarity_matrix_custom(func, embedding1, embedding2, chunk_size = 1000):\n",
        "    rows = embedding1.shape[0]\n",
        "\n",
        "    num_chunks = int(np.ceil(rows / chunk_size))\n",
        "\n",
        "    for i in range(num_chunks):\n",
        "        end_chunk = (i+1)*(chunk_size) if (i+1)*(chunk_size) < rows else rows\n",
        "        yield func(embedding1[i*chunk_size:end_chunk,:], embedding2)\n",
        "\n",
        "text_chem_cos = memory_efficient_similarity_matrix_custom(cosine_similarity, text_embeddings_train, all_mol_embeddings)\n",
        "\n",
        "# Calculate Ranks:\n",
        "tr_avg_ranks = np.zeros((n_train, n))\n",
        "\n",
        "ranks_train = []\n",
        "cosine_vs_rank = []\n",
        "\n",
        "def get_ranks(text_chem_cos, ranks_avg, offset, split= \"\"):\n",
        "    ranks_tmp = []\n",
        "    j = 0 #keep track of all loops\n",
        "    for l, emb in enumerate(text_chem_cos):\n",
        "        for k in range(emb.shape[0]):\n",
        "            cid_locs = np.argsort(emb[k,:])[::-1]\n",
        "            ranks = np.argsort(cid_locs)\n",
        "\n",
        "            ranks_avg[j,:] = ranks_avg[j,:] + ranks\n",
        "\n",
        "            rank = ranks[j+offset] + 1\n",
        "            ranks_tmp.append(rank)\n",
        "\n",
        "            cosine_vs_rank.append((np.max(emb[k,:]), rank))\n",
        "\n",
        "            j += 1\n",
        "            if j % 1000 == 0: print(j, split+\" processed\")\n",
        "\n",
        "    return np.array(ranks_tmp)\n",
        "\n",
        "ranks_tmp = get_ranks(text_chem_cos, tr_avg_ranks, offset=0, split=\"train\")\n",
        "ranks_train = ranks_tmp\n",
        "cosines = [cr[0] for cr in cosine_vs_rank]\n",
        "ranks = [cr[1] for cr in cosine_vs_rank]\n",
        "\n",
        "plt.scatter(cosines, ranks)\n",
        "plt.xlabel('Cosine Similarity')\n",
        "plt.ylabel('Ranks')\n",
        "plt.savefig(output_file)\n",
        "# Uncomment if you want to show the plot\n",
        "# plt.show()"
      ],
      "metadata": {
        "id": "k1-e5bXApQG2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "For the code above, here is an example of a graph that is generated for plotting the cosine score vs embeddings for the MLP model that was trained for 2 epochs with batch size 32. Note that this only takes into account the \"train\" rankings as mentioned in the code above\n",
        "\n",
        "![image.png](data:image/png;base64,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)\n",
        "\n"
      ],
      "metadata": {
        "id": "d8oTYc1rHbMn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Ensemble output based on multiple embeddings.\n",
        "# For brevity purposes, the code below generates the ensemble output for only\n",
        "# the training embeddings\n",
        "'''\n",
        "inputs = \"../softmax_CLIP/embeddings\"\n",
        "dir1 = osp.join(inputs, \"GCN1/\")\n",
        "dir2 = osp.join(inputs, \"GCN2/\")\n",
        "dir3 = osp.join(inputs, \"GCN3/\")\n",
        "dir4 = osp.join(inputs, \"MLP1/\")\n",
        "dir5 = osp.join(inputs, \"MLP2/\")\n",
        "dir6 = osp.join(inputs, \"MLP3/\")\n",
        "dirs = [dir1, dir2, dir3, dir4, dir5, dir6]\n",
        "'''\n",
        "\n",
        "NUM_MODELS = len(dirs)\n",
        "\n",
        "# Do similar for validation and testing if needed.\n",
        "text_embeddings_train = []\n",
        "chem_embeddings_train = []\n",
        "\n",
        "# Declare arrays for validation and testing if needed.\n",
        "cids_train = []\n",
        "\n",
        "for i, dir in enumerate(dirs):\n",
        "    cids_train.append(np.load(osp.join(dir, \"cids_train.npy\"), allow_pickle=True))\n",
        "    text_embeddings_train.append(np.load(osp.join(dir, \"text_embeddings_train.npy\")))\n",
        "    chem_embeddings_train.append(np.load(osp.join(dir, \"chem_embeddings_train.npy\")))\n",
        "    print('Loaded embedding from model', i+1)\n",
        "\n",
        "for i in range(1, NUM_MODELS):\n",
        "    tmp = cids_train[i].tolist()\n",
        "    indexes = [tmp.index(i) for i in cids_train[0]]\n",
        "\n",
        "    cids_train[i] = cids_train[i][indexes]\n",
        "\n",
        "    text_embeddings_train[i] = text_embeddings_train[i][indexes]\n",
        "\n",
        "    chem_embeddings_train[i] = chem_embeddings_train[i][indexes]\n",
        "\n",
        "    print('Embeddings {} reordered'.format(i+1))\n",
        "\n",
        "print('Sorted embeddings')\n",
        "\n",
        "#combine all splits:\n",
        "all_text_embbedings = []\n",
        "all_mol_embeddings = []\n",
        "for i in range(NUM_MODELS):\n",
        "    all_text_embbedings.append(np.concatenate(text_embeddings_train[i], axis = 0))\n",
        "    all_mol_embeddings.append(np.concatenate(chem_embeddings_train[i], axis = 0))\n",
        "\n",
        "all_cids = np.concatenate(cids_train[0], axis = 0)\n",
        "\n",
        "n_train = len(cids_train[0])\n",
        "n = n_train\n",
        "\n",
        "def memory_efficient_similarity_matrix_custom(func, embedding1, embedding2, chunk_size = 1000):\n",
        "    rows = embedding1.shape[0]\n",
        "\n",
        "    num_chunks = int(np.ceil(rows / chunk_size))\n",
        "\n",
        "    for i in range(num_chunks):\n",
        "        end_chunk = (i+1)*(chunk_size) if (i+1)*(chunk_size) < rows else rows\n",
        "        yield func(embedding1[i*chunk_size:end_chunk,:], embedding2)\n",
        "\n",
        "text_chem_cos = []\n",
        "text_chem_cos_val = []\n",
        "text_chem_cos_test = []\n",
        "for i in range(NUM_MODELS):\n",
        "    text_chem_cos.append(memory_efficient_similarity_matrix_custom(cosine_similarity, text_embeddings_train[i], all_mol_embeddings[i]))\n",
        "    text_chem_cos_val.append(memory_efficient_similarity_matrix_custom(cosine_similarity, text_embeddings_val[i], all_mol_embeddings[i]))\n",
        "    text_chem_cos_test.append(memory_efficient_similarity_matrix_custom(cosine_similarity, text_embeddings_test[i], all_mol_embeddings[i]))\n",
        "\n",
        "tr_avg_ranks = np.zeros((n_train, n))\n",
        "ranks_train = []\n",
        "\n",
        "def get_ranks(text_chem_cos, ranks_avg, offset, split= \"\"):\n",
        "    ranks_tmp = []\n",
        "    j = 0 #keep track of all loops\n",
        "    for l, emb in enumerate(text_chem_cos):\n",
        "        for k in range(emb.shape[0]):\n",
        "            cid_locs = np.argsort(emb[k,:])[::-1]\n",
        "            ranks = np.argsort(cid_locs)\n",
        "\n",
        "            ranks_avg[j,:] = ranks_avg[j,:] + ranks\n",
        "\n",
        "            rank = ranks[j+offset] + 1\n",
        "            ranks_tmp.append(rank)\n",
        "\n",
        "            j += 1\n",
        "            if j % 1000 == 0: print(j, split+\" processed\")\n",
        "\n",
        "    return np.array(ranks_tmp)\n",
        "\n",
        "def print_ranks(ranks, model_num, split):\n",
        "\n",
        "    print(split+\" Model {}:\".format(model_num))\n",
        "    print(\"Mean rank:\", np.mean(ranks))\n",
        "    print(\"Hits at 1:\", np.mean(ranks <= 1))\n",
        "    print(\"Hits at 10:\", np.mean(ranks <= 10))\n",
        "    print(\"Hits at 100:\", np.mean(ranks <= 100))\n",
        "    print(\"Hits at 500:\", np.mean(ranks <= 500))\n",
        "    print(\"Hits at 1000:\", np.mean(ranks <= 1000))\n",
        "\n",
        "    print(\"MRR:\", np.mean(1/ranks))\n",
        "    print()\n",
        "\n",
        "for i in range(NUM_MODELS):\n",
        "    ranks_tmp = get_ranks(text_chem_cos[i], tr_avg_ranks, offset=0, split=\"train\")\n",
        "    print_ranks(ranks_tmp, i+1, split=\"Training\")\n",
        "    ranks_train.append(ranks_tmp)\n",
        "\n",
        "#Process ensemble:\n",
        "sorted = np.argsort(tr_avg_ranks)\n",
        "new_tr_ranks = np.diag(np.argsort(sorted)) + 1\n",
        "\n",
        "print_ranks(new_tr_ranks, \"e\", split=\"Training Ensemble\")"
      ],
      "metadata": {
        "id": "G1cl0t85cpEc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here is example output of the ensemble command, for the MLP Model and GCN Model. The ouputs here are for the two models that have been trained for 2 epochs with batch_size as 32.\n",
        "\n",
        "Training Model 1:\n",
        "Mean rank: 299.47837776431385\n",
        "Hits at 1: 0.01673735231747955\n",
        "Hits at 10: 0.12799151772190245\n",
        "Hits at 100: 0.5427900636170857\n",
        "Hits at 500: 0.8639427446228416\n",
        "Hits at 1000: 0.938579218418661\n",
        "MRR: 0.057560443722529685\n",
        "\n",
        "Training Model 2:\n",
        "Mean rank: 375.6514692517419\n",
        "Hits at 1: 0.01219327476522266\n",
        "Hits at 10: 0.09622084216903969\n",
        "Hits at 100: 0.4447137231142078\n",
        "Hits at 500: 0.8117615873977583\n",
        "Hits at 1000: 0.920365040896698\n",
        "MRR: 0.044266726660049036"
      ],
      "metadata": {
        "id": "X0KYRlTJgsOJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "## Model comparison"
      ],
      "metadata": {
        "id": "8EAWAy_LwHlV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# compare you model with others\n",
        "# you don't need to re-run all other experiments, instead, you can directly refer the metrics/numbers in the paper"
      ],
      "metadata": {
        "id": "uOdhGrbwwG71"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Discussion - Instructions\n",
        "\n",
        "**TODO:** *remove this before submitting*\n",
        "\n",
        "In this section,you should discuss your work and make future plan. The discussion should address the following questions:\n",
        "  * Make assessment that the paper is reproducible or not.\n",
        "  * Explain why it is not reproducible if your results are kind negative.\n",
        "  * Describe What was easy and What was difficult during the reproduction.\n",
        "  * Make suggestions to the author or other reproducers on how to improve the reproducibility.\n",
        "  * What will you do in next phase.\n",
        "\n"
      ],
      "metadata": {
        "id": "qH75TNU71eRH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Discussion\n",
        "\n",
        "The code in the paper was reproducible although we had great difficulty with one portion (extracting the multi-head attention). There are aspects of the paper (specifically calling out MLP1, 2, 3 and GCN1, 2, 3) that are quite confusing.\n",
        "\n",
        "## What was easy?\n",
        "\n",
        "* The authors made it easy with both Python and Jupyter notebooks.\n",
        "* While the dataset was mislabeled in the GitHub repository, it was fully available.\n",
        "* The paper and the subject are approachable.\n",
        "\n",
        "## What was difficult?\n",
        "\n",
        "* The code from the authors was mostly uncommented and generally poorly written. In particular, code was duplicated in numerous places with minor, unexplained differences.\n",
        "* One particular section of code extracting the multi-head attention weights relied upon an older form of PyTorch. The code was extremely poorly commented, so it was hard to understand what the authors intended. This took the better part of a week to debug. If the authors had supplied a requirements.txt file, it would have been easier to fix.\n",
        "* The code was additionally extremely unoptimized. A minor tweak to the loss computation by keeping the values on the GPU decreased step time by 10%.\n",
        "\n",
        "## Suggestions to the authors\n",
        "\n",
        "* Functions are your friends. Inheritance is even better. Try refactoring your code to make it easier to understand and profile.\n",
        "* Try performance profiling your code, you will find multiple instances where you can get 10% to 3x performance improvements.\n",
        "* The paper and the code does not adequately explain MLP1, MLP2, MLP3 and GCN1, GCN2, GCN3.\n",
        "\n",
        "## Next phase\n",
        "\n",
        "In the final phase of the project, we want to finish up the code to include all of the association rules.\n",
        "\n",
        "Originaly we planned 5 different ablations\n",
        "1. Use BERT instead of SciBERT to gauge the impact.\n",
        "1. Remove the learned temperature parameter from the general loss function to gauge the impact.\n",
        "1. Remove negative sampling from the loss function for the cross-modal attention model to gauge the impact.\n",
        "1. Use one token for each atom (r = 0) instead of two to gauge the impact.\n",
        "1. Remove layer normalization from the encoders to gauge the impact.\n",
        "\n",
        "We will attempt each of these to see the difference on the final results."
      ],
      "metadata": {
        "id": "Am1IHemLLV5R"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# References\n",
        "\n",
        "1. Edwards, Carl & Zhai, ChengXiang & Ji, Heng. (2021). Text2Mol: Cross-Modal Molecule Retrieval with Natural Language Queries. 595-607. 10.18653/v1/2021.emnlp-main.47.\n",
        "\n",
        "1. Beltagy, I., Lo, K., & Cohan, A. (2019). SciBERT: A Pretrained Language Model for Scientific Text. Conference on Empirical Methods in Natural Language Processing.\n",
        "\n",
        "1. Jaeger, Sabrina and Fulle, Simone and Turk, Samo. (2018) Mol2vec: Unsupervised Machine Learning Approach with Chemical Intuition. Journal of Chemical Information and Modeling 10.1021/acs.jcim.7b00616\n",
        "\n",
        "1. Chemical Entities of Biological Interest (ChEBI) https://www.ebi.ac.uk/chebi/downloadsForward.do\n",
        "\n",
        "1. GitHub Text2Mol repository: https://github.com/cnedwards/text2mol\n",
        "\n"
      ],
      "metadata": {
        "id": "SHMI2chl9omn"
      }
    }
  ]
}