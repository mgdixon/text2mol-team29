{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "V100",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mgdixon/text2mol-team29/blob/main/MLP_Ablations.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Instructions\n",
        "\n",
        "This file is for the MLP flavor of the ablations. Some of the ablations are not relevant for MLP. For example, the negative sampling loss. However, for consistency sake, we keep all of the options. Using the global variables below, pick one of the ablations to run and give the model a name. The actual ablations are below:\n",
        "\n",
        "1. Use BERT instead of SciBERT to gauge the impact.\n",
        "1. Remove the learned temperature parameter from the general loss function to gauge the impact.\n",
        "1. Remove negative sampling from the loss function for the cross-modal attention model to gauge the impact.\n",
        "1. Use one token for each atom (r = 0) instead of two to gauge the impact.\n",
        "1. Remove layer normalization from the encoders to gauge the impact."
      ],
      "metadata": {
        "id": "JxobBaizNERI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "ABLATION_1 = False\n",
        "ABLATION_2 = True\n",
        "ABLATION_3 = False\n",
        "ABLATION_4 = False\n",
        "ABLATION_5 = False\n",
        "MODEL = \"MLP\"\n",
        "\n"
      ],
      "metadata": {
        "id": "VmciT1GKyt-c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OVnGSv6cxVKj"
      },
      "source": [
        "# Authenticate.\n",
        "from google.colab import auth\n",
        "auth.authenticate_user()\n",
        "\n",
        "# Install Cloud Storage FUSE.\n",
        "!echo \"deb https://packages.cloud.google.com/apt gcsfuse-`lsb_release -c -s` main\" | sudo tee /etc/apt/sources.list.d/gcsfuse.list\n",
        "!curl https://packages.cloud.google.com/apt/doc/apt-key.gpg | sudo apt-key add -\n",
        "!apt -qq update && apt -qq install gcsfuse"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Mount a Cloud Storage bucket or location, without the gs:// prefix.\n",
        "mount_path = \"team29-text2mol\"  # or a location like \"my-bucket/path/to/mount\"\n",
        "local_path = f\"/mnt/gs/{mount_path}\"\n",
        "\n",
        "!mkdir -p {local_path}\n",
        "!gcsfuse --implicit-dirs {mount_path} {local_path}"
      ],
      "metadata": {
        "id": "GXYIhSunIsaL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Then you can access it like a local path.\n",
        "!ls -lh {local_path}/outputs/"
      ],
      "metadata": {
        "id": "6DQHdGf2IxOf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_XgTpm9ZxoN9"
      },
      "source": [
        "import os\n",
        "import shutil\n",
        "import time\n",
        "\n",
        "import math\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "import torch\n",
        "from torch import nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "import tokenizers\n",
        "from tokenizers import Tokenizer\n",
        "from transformers import BertTokenizerFast, BertModel\n",
        "\n",
        "import csv"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if ABLATION_1:\n",
        "  output_path = local_path + \"/\" + MODEL + \"_Ablation_1/\"\n",
        "elif ABLATION_2:\n",
        "  output_path = local_path + \"/\" + MODEL + \"_Ablation_2/\"\n",
        "elif ABLATION_3:\n",
        "  output_path = local_path + \"/\" + MODEL + \"_Ablation_3/\"\n",
        "elif ABLATION_4:\n",
        "  output_path = local_path + \"/\" + MODEL + \"_Ablation_4/\"\n",
        "elif ABLATION_5:\n",
        "  output_path = local_path + \"/\" + MODEL + \"_Ablation_5/\"\n",
        "else:\n",
        "  output_path = local_path + \"/\" + MODEL + \"_NoAblation/\"\n",
        "\n",
        "emb_path = output_path + \"embeddings/\"\n",
        "\n",
        "if not os.path.exists(output_path):\n",
        "  os.mkdir(output_path)\n",
        "\n",
        "if not os.path.exists(emb_path):\n",
        "  os.mkdir(emb_path)\n"
      ],
      "metadata": {
        "id": "nbY82j5a3d2v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5gpao3ymyTBg"
      },
      "source": [
        "#Need a special generator for random sampling:\n",
        "\n",
        "class GenerateData():\n",
        "  def __init__(self, path_train, path_val, path_test, path_molecules,\n",
        "               path_token_embs, ablation_1 = False):\n",
        "\n",
        "    self.path_train = path_train\n",
        "    self.path_val = path_val\n",
        "    self.path_test = path_test\n",
        "    self.path_molecules = path_molecules\n",
        "    self.path_token_embs = path_token_embs\n",
        "    self.ablation_1 = ablation_1\n",
        "\n",
        "    self.text_trunc_length = 256\n",
        "\n",
        "    self.prep_text_tokenizer()\n",
        "\n",
        "    self.load_substructures()\n",
        "\n",
        "    self.batch_size = 32\n",
        "\n",
        "    self.store_descriptions()\n",
        "\n",
        "  def load_substructures(self):\n",
        "    self.molecule_sentences = {}\n",
        "    self.molecule_tokens = {}\n",
        "\n",
        "    total_tokens = set()\n",
        "    self.max_mol_length = 0\n",
        "    with open(self.path_molecules) as f:\n",
        "      for line in f:\n",
        "        spl = line.split(\":\")\n",
        "        cid = spl[0]\n",
        "        tokens = spl[1].strip()\n",
        "        self.molecule_sentences[cid] = tokens\n",
        "        t = tokens.split()\n",
        "        total_tokens.update(t)\n",
        "        size = len(t)\n",
        "        if size > self.max_mol_length: self.max_mol_length = size\n",
        "\n",
        "\n",
        "    self.token_embs = np.load(self.path_token_embs, allow_pickle = True)[()]\n",
        "\n",
        "  def prep_text_tokenizer(self):\n",
        "    if self.ablation_1:\n",
        "      self.text_tokenizer = BertTokenizerFast.from_pretrained(\"google-bert/bert-base-uncased\")\n",
        "    else:\n",
        "      self.text_tokenizer = BertTokenizerFast.from_pretrained(\"allenai/scibert_scivocab_uncased\")\n",
        "\n",
        "  # Refactored version that's much shorter\n",
        "  def read_file(self, file_path):\n",
        "    cids = []\n",
        "    with open(file_path) as f:\n",
        "        reader = csv.DictReader(f, delimiter=\"\\t\", quoting=csv.QUOTE_NONE,\n",
        "                                fieldnames=['cid', 'mol2vec', 'desc'])\n",
        "        for line in reader:\n",
        "            self.descriptions[line['cid']] = line['desc']\n",
        "            self.mols[line['cid']] = line['mol2vec']\n",
        "            cids.append(line['cid'])\n",
        "    return cids\n",
        "\n",
        "  def store_descriptions(self):\n",
        "    self.descriptions = {}\n",
        "    self.mols = {}\n",
        "    self.training_cids = self.read_file(self.path_train)\n",
        "    self.validation_cids = self.read_file(self.path_val)\n",
        "    self.test_cids = self.read_file(self.path_test)\n",
        "\n",
        "  # Refactored version\n",
        "  def generate_examples(self, cids):\n",
        "    \"\"\"Yields examples.\"\"\"\n",
        "\n",
        "    np.random.shuffle(cids)\n",
        "\n",
        "    for cid in cids:\n",
        "      text_input = self.text_tokenizer(self.descriptions[cid], truncation=True,\n",
        "                                       max_length=self.text_trunc_length,\n",
        "                                       padding='max_length',\n",
        "                                       return_tensors = 'np')\n",
        "\n",
        "      yield {\n",
        "          'cid': cid,\n",
        "          'input': {\n",
        "              'text': {\n",
        "                'input_ids': text_input['input_ids'].squeeze(),\n",
        "                'attention_mask': text_input['attention_mask'].squeeze(),\n",
        "                'original_text': self.descriptions[cid]\n",
        "              },\n",
        "              'molecule' : {\n",
        "                    'mol2vec' : np.fromstring(self.mols[cid], sep = \" \"),\n",
        "                    'cid' : cid\n",
        "              },\n",
        "          },\n",
        "      }\n",
        "\n",
        "  def generate_examples_train(self):\n",
        "    yield from self.generate_examples(self.training_cids)\n",
        "\n",
        "  def generate_examples_val(self):\n",
        "    yield from self.generate_examples(self.validation_cids)\n",
        "\n",
        "  def generate_examples_test(self):\n",
        "    yield from self.generate_examples(self.test_cids)\n",
        "\n",
        "  def generate_examples_custom(self, custom_cids):\n",
        "    yield from self.generate_examples(custom_cids)\n",
        "\n",
        "data_path = local_path + \"/data/\"\n",
        "mounted_path_token_embs = data_path + \"token_embedding_dict.npy\"\n",
        "mounted_path_train = data_path + \"training.txt\"\n",
        "mounted_path_val = data_path + \"val.txt\"\n",
        "mounted_path_test = data_path + \"test.txt\"\n",
        "mounted_path_molecules = data_path + \"ChEBI_defintions_substructure_corpus.cp\"\n",
        "gt = GenerateData(mounted_path_train, mounted_path_val, mounted_path_test,\n",
        "                  mounted_path_molecules, mounted_path_token_embs,\n",
        "                  ablation_1 = ABLATION_1)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zck-zGTa8JOv"
      },
      "source": [
        "\n",
        "\n",
        "class Dataset(Dataset):\n",
        "  'Characterizes a dataset for PyTorch'\n",
        "  def __init__(self, gen, length):\n",
        "      'Initialization'\n",
        "\n",
        "      self.gen = gen\n",
        "      self.it = iter(self.gen())\n",
        "\n",
        "      self.length = length\n",
        "\n",
        "  def __len__(self):\n",
        "      'Denotes the total number of samples'\n",
        "      return self.length\n",
        "\n",
        "\n",
        "  def __getitem__(self, index):\n",
        "      'Generates one sample of data'\n",
        "\n",
        "      try:\n",
        "        ex = next(self.it)\n",
        "      except StopIteration:\n",
        "        self.it = iter(self.gen())\n",
        "        ex = next(self.it)\n",
        "\n",
        "      X = ex['input']\n",
        "      y = 1\n",
        "\n",
        "      return X, y\n",
        "\n",
        "training_set = Dataset(gt.generate_examples_train, len(gt.training_cids))\n",
        "validation_set = Dataset(gt.generate_examples_val, len(gt.validation_cids))\n",
        "test_set = Dataset(gt.generate_examples_test, len(gt.test_cids))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7Fj8h8vhk3W0"
      },
      "source": [
        "\n",
        "# Parameters\n",
        "params = {'batch_size': gt.batch_size,\n",
        "          'shuffle': True,\n",
        "          'num_workers': 1}\n",
        "\n",
        "training_generator = DataLoader(training_set, **params)\n",
        "validation_generator = DataLoader(validation_set, **params)\n",
        "test_generator = DataLoader(test_set, **params)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aksj743St9ga"
      },
      "source": [
        "\n",
        "class MLPModel(nn.Module):\n",
        "    def __init__(self, ntoken, ninp, nout, nhid, dropout=0.5,\n",
        "                 ablation_1 = False, ablation_2 = False, ablation_3 = False,\n",
        "                 ablation_4 = False, ablation_5 = False):\n",
        "        super(MLPModel, self).__init__()\n",
        "\n",
        "        # This could be a list, but the code is simpler to read using\n",
        "        # discrete variables\n",
        "        self.ablation_1 = ablation_1 # BERT not SciBert\n",
        "        self.ablation_2 = ablation_2 # Remove temperature\n",
        "        self.ablation_3 = ablation_3 # Remove negative sampling\n",
        "        self.ablation_4 = ablation_4 # One token per atom?\n",
        "        self.ablation_5 = ablation_5 # Remove layer norm\n",
        "\n",
        "        self.text_hidden1 = nn.Linear(ninp, nout)\n",
        "\n",
        "        self.ninp = ninp\n",
        "        self.nhid = nhid\n",
        "        self.nout = nout\n",
        "\n",
        "        self.drop = nn.Dropout(p=dropout)\n",
        "\n",
        "        self.mol_hidden1 = nn.Linear(nout, nhid)\n",
        "        self.mol_hidden2 = nn.Linear(nhid, nhid)\n",
        "        self.mol_hidden3 = nn.Linear(nhid, nout)\n",
        "\n",
        "        # Ablation 2 removes the learned temperature\n",
        "        if not self.ablation_2:\n",
        "          self.temp = nn.Parameter(torch.Tensor([0.07]))\n",
        "          self.register_parameter( 'temp' , self.temp )\n",
        "\n",
        "        # Ablation 5 removes the layer normalization\n",
        "        if not self.ablation_5:\n",
        "          self.ln1 = nn.LayerNorm((nout))\n",
        "          self.ln2 = nn.LayerNorm((nout))\n",
        "\n",
        "        self.relu = nn.ReLU()\n",
        "        self.selu = nn.SELU()\n",
        "\n",
        "        self.other_params = list(self.parameters()) #get all but bert params\n",
        "\n",
        "        # Ablation 1 chooses between the SciBert model and a base BERT model\n",
        "        if self.ablation_1:\n",
        "          self.text_transformer_model = BertModel.from_pretrained(\"google-bert/bert-base-uncased\")\n",
        "        else:\n",
        "          self.text_transformer_model = BertModel.from_pretrained(\"allenai/scibert_scivocab_uncased\")\n",
        "\n",
        "        self.text_transformer_model.train()\n",
        "\n",
        "    def forward(self, text, molecule, text_mask = None, molecule_mask = None):\n",
        "\n",
        "        text_encoder_output = self.text_transformer_model(text, attention_mask = text_mask)\n",
        "\n",
        "        text_x = text_encoder_output['pooler_output']\n",
        "        text_x = self.text_hidden1(text_x)\n",
        "\n",
        "        x = self.relu(self.mol_hidden1(molecule))\n",
        "        x = self.relu(self.mol_hidden2(x))\n",
        "        x = self.mol_hidden3(x)\n",
        "\n",
        "        # Ablation 5 removes the layer normalization\n",
        "        if not self.ablation_5:\n",
        "          x = self.ln1(x)\n",
        "          text_x = self.ln2(text_x)\n",
        "\n",
        "        # Ablation 2 removes the learned temperature\n",
        "        if not self.ablation_2:\n",
        "          x = x * torch.exp(self.temp)\n",
        "          text_x = text_x * torch.exp(self.temp)\n",
        "\n",
        "        return text_x, x\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mGMF8AZcB2Zy"
      },
      "source": [
        "model = MLPModel(ntoken = gt.text_tokenizer.vocab_size, ninp = 768, nhid = 600,\n",
        "                 nout = 300, ablation_1 = ABLATION_1, ablation_2 = ABLATION_2,\n",
        "                 ablation_3 = ABLATION_3, ablation_4 = ABLATION_4,\n",
        "                 ablation_5 = ABLATION_5)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P9eP2y9dbw32"
      },
      "source": [
        "import torch.optim as optim\n",
        "from transformers.optimization import get_linear_schedule_with_warmup\n",
        "\n",
        "epochs = 40\n",
        "\n",
        "init_lr = 1e-4\n",
        "bert_lr = 3e-5\n",
        "bert_params = list(model.text_transformer_model.parameters())\n",
        "\n",
        "optimizer = optim.Adam([\n",
        "                {'params': model.other_params},\n",
        "                {'params': bert_params, 'lr': bert_lr}\n",
        "            ], lr=init_lr)\n",
        "\n",
        "num_warmup_steps = 1000\n",
        "num_training_steps = epochs * len(training_generator) - num_warmup_steps\n",
        "scheduler = get_linear_schedule_with_warmup(optimizer,\n",
        "                                            num_warmup_steps = num_warmup_steps,\n",
        "                                            num_training_steps = num_training_steps)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-1heECu1nVRB"
      },
      "source": [
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "print(device)\n",
        "\n",
        "tmp = model.to(device)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HytSaAyHNBuZ"
      },
      "source": [
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "def loss_func(v1, v2):\n",
        "  logits = torch.matmul(v1,torch.transpose(v2, 0, 1))\n",
        "  labels = torch.arange(logits.shape[0]).to(device)\n",
        "  return criterion(logits, labels) + criterion(torch.transpose(logits, 0, 1),\n",
        "                                               labels)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HtfDFAnN_Neu"
      },
      "source": [
        "train_losses = []\n",
        "val_losses = []\n",
        "\n",
        "train_acc = []\n",
        "val_acc = []\n",
        "\n",
        "\n",
        "\n",
        "# Loop over epochs\n",
        "for epoch in range(epochs):\n",
        "    # Training\n",
        "\n",
        "    start_time = time.time()\n",
        "    running_loss = 0.0\n",
        "    running_acc = 0.0\n",
        "    # Keep the losses on the GPU\n",
        "    running_loss_gpu = torch.tensor(0.0).to(device)\n",
        "\n",
        "    model.train()\n",
        "    for i, d in enumerate(training_generator):\n",
        "        batch, labels = d\n",
        "        # Transfer to GPU\n",
        "\n",
        "        text_mask = batch['text']['attention_mask'].bool()\n",
        "\n",
        "        text = batch['text']['input_ids'].to(device)\n",
        "        text_mask = text_mask.to(device)\n",
        "        molecule = batch['molecule']['mol2vec'].float().to(device)\n",
        "\n",
        "        text_out, chem_out = model(text, molecule, text_mask)\n",
        "\n",
        "        loss = loss_func(text_out, chem_out).to(device)\n",
        "\n",
        "        running_loss_gpu += loss\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        scheduler.step()\n",
        "\n",
        "        if (i+1) % 100 == 0:\n",
        "          running_loss = running_loss_gpu.item()\n",
        "          print(i+1, \"batches trained. Avg loss:\\t\", running_loss / (i+1),\n",
        "                \". Avg ms/step =\", 1000*(time.time()-start_time)/(i+1))\n",
        "\n",
        "    running_loss = running_loss_gpu.item()\n",
        "    train_losses.append(running_loss / (i+1))\n",
        "    train_acc.append(running_acc / (i+1))\n",
        "\n",
        "    print(\"Epoch\", epoch, \"training loss:\\t\\t\", running_loss / (i+1),\n",
        "          \". Time =\", (time.time()-start_time), \"seconds.\")\n",
        "\n",
        "\n",
        "    # Validation\n",
        "    model.eval()\n",
        "    with torch.set_grad_enabled(False):\n",
        "      start_time = time.time()\n",
        "      running_acc = 0.0\n",
        "      running_loss = 0.0\n",
        "      # Keep the losses on the GPU\n",
        "      running_loss_gpu = torch.tensor(0.0).to(device)\n",
        "\n",
        "      for i, d in enumerate(validation_generator):\n",
        "          batch, labels = d\n",
        "          # Transfer to GPU\n",
        "\n",
        "          text_mask = batch['text']['attention_mask'].bool()\n",
        "\n",
        "          text = batch['text']['input_ids'].to(device)\n",
        "          text_mask = text_mask.to(device)\n",
        "          molecule = batch['molecule']['mol2vec'].float().to(device)\n",
        "\n",
        "\n",
        "\n",
        "          text_out, chem_out = model(text, molecule, text_mask)\n",
        "\n",
        "          loss = loss_func(text_out, chem_out).to(device)\n",
        "          running_loss_gpu += loss\n",
        "\n",
        "          if (i+1) % 100 == 0:\n",
        "            running_loss = running_loss_gpu.item()\n",
        "            print(i+1, \"batches eval. Avg loss:\\t\", running_loss / (i+1),\n",
        "                  \". Avg ms/step =\", 1000*(time.time()-start_time)/(i+1))\n",
        "\n",
        "      running_loss = running_loss_gpu.item()\n",
        "      val_losses.append(running_loss / (i+1))\n",
        "      val_acc.append(running_acc / (i+1))\n",
        "\n",
        "\n",
        "      min_loss = np.min(val_losses)\n",
        "      if val_losses[-1] == min_loss:\n",
        "          torch.save(model.state_dict(),\n",
        "                     output_path + \\\n",
        "                     'weights_pretrained.{epoch:02d}-{min_loss:.2f}.pt'\\\n",
        "                     .format(epoch = epoch, min_loss = min_loss))\n",
        "\n",
        "    print(\"Epoch\", epoch, \"validation loss:\\t\", running_loss / (i+1),\n",
        "          \". Time =\", (time.time()-start_time), \"seconds.\")\n",
        "\n",
        "\n",
        "torch.save(model.state_dict(), output_path + \"final_weights.\"+str(epochs)+\".pt\")\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BzSbNJ4fWGN6"
      },
      "source": [
        "## Extract Embeddings\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oCP_DuaaWKtV"
      },
      "source": [
        "cids_train = np.array([])\n",
        "cids_val = np.array([])\n",
        "cids_test = np.array([])\n",
        "chem_embeddings_train = np.array([])\n",
        "text_embeddings_train = np.array([])\n",
        "chem_embeddings_val = np.array([])\n",
        "text_embeddings_val = np.array([])\n",
        "chem_embeddings_test = np.array([])\n",
        "text_embeddings_test = np.array([])\n",
        "\n",
        "with torch.no_grad():\n",
        "  for i, d in enumerate(gt.generate_examples_train()):\n",
        "    cid = np.array([d['cid']])\n",
        "    text_mask = torch.Tensor(d['input']['text']['attention_mask']).bool().reshape(1,-1).to(device)\n",
        "\n",
        "    text = torch.Tensor(d['input']['text']['input_ids']).int().reshape(1,-1).to(device)\n",
        "    molecule = torch.Tensor(d['input']['molecule']['mol2vec']).float().reshape(1,-1).to(device)\n",
        "    text_emb, chem_emb = model(text, molecule, text_mask)\n",
        "\n",
        "    chem_emb = chem_emb.cpu().numpy()\n",
        "    text_emb = text_emb.cpu().numpy()\n",
        "\n",
        "    cids_train = np.concatenate((cids_train, cid)) if cids_train.size else cid\n",
        "    chem_embeddings_train = np.concatenate((chem_embeddings_train, chem_emb)) if chem_embeddings_train.size else chem_emb\n",
        "    text_embeddings_train = np.concatenate((text_embeddings_train, text_emb)) if text_embeddings_train.size else text_emb\n",
        "\n",
        "    if (i+1) % 100 == 0: print(i+1, \"samples eval.\")\n",
        "\n",
        "\n",
        "  print(cids_train.shape, chem_embeddings_train.shape)\n",
        "\n",
        "  for d in gt.generate_examples_val():\n",
        "    cid = np.array([d['cid']])\n",
        "    text_mask = torch.Tensor(d['input']['text']['attention_mask']).bool().reshape(1,-1).to(device)\n",
        "\n",
        "    text = torch.Tensor(d['input']['text']['input_ids']).int().reshape(1,-1).to(device)\n",
        "    molecule = torch.Tensor(d['input']['molecule']['mol2vec']).float().reshape(1,-1).to(device)\n",
        "    text_emb, chem_emb = model(text, molecule, text_mask)\n",
        "\n",
        "    chem_emb = chem_emb.cpu().numpy()\n",
        "    text_emb = text_emb.cpu().numpy()\n",
        "\n",
        "    cids_val = np.concatenate((cids_val, cid)) if cids_val.size else cid\n",
        "    chem_embeddings_val = np.concatenate((chem_embeddings_val, chem_emb)) if chem_embeddings_val.size else chem_emb\n",
        "    text_embeddings_val = np.concatenate((text_embeddings_val, text_emb)) if text_embeddings_val.size else text_emb\n",
        "\n",
        "  print(cids_val.shape, chem_embeddings_val.shape)\n",
        "\n",
        "  for d in gt.generate_examples_test():\n",
        "    cid = np.array([d['cid']])\n",
        "    text_mask = torch.Tensor(d['input']['text']['attention_mask']).bool().reshape(1,-1).to(device)\n",
        "\n",
        "    text = torch.Tensor(d['input']['text']['input_ids']).int().reshape(1,-1).to(device)\n",
        "    molecule = torch.Tensor(d['input']['molecule']['mol2vec']).float().reshape(1,-1).to(device)\n",
        "    text_emb, chem_emb = model(text, molecule, text_mask)\n",
        "\n",
        "    chem_emb = chem_emb.cpu().numpy()\n",
        "    text_emb = text_emb.cpu().numpy()\n",
        "\n",
        "    cids_test = np.concatenate((cids_test, cid)) if cids_test.size else cid\n",
        "    chem_embeddings_test = np.concatenate((chem_embeddings_test, chem_emb)) if chem_embeddings_test.size else chem_emb\n",
        "    text_embeddings_test = np.concatenate((text_embeddings_test, text_emb)) if text_embeddings_test.size else text_emb\n",
        "\n",
        "print(cids_test.shape, chem_embeddings_test.shape)\n",
        "\n",
        "\n",
        "np.save(emb_path + \"cids_train.npy\", cids_train)\n",
        "np.save(emb_path + \"cids_val.npy\", cids_val)\n",
        "np.save(emb_path + \"cids_test.npy\", cids_test)\n",
        "np.save(emb_path + \"chem_embeddings_train.npy\", chem_embeddings_train)\n",
        "np.save(emb_path + \"chem_embeddings_val.npy\", chem_embeddings_val)\n",
        "np.save(emb_path + \"chem_embeddings_test.npy\", chem_embeddings_test)\n",
        "np.save(emb_path + \"text_embeddings_train.npy\", text_embeddings_train)\n",
        "np.save(emb_path + \"text_embeddings_val.npy\", text_embeddings_val)\n",
        "np.save(emb_path + \"text_embeddings_test.npy\", text_embeddings_test)\n"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}